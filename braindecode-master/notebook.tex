
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Cropped\_Decoding-units}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
        \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{n}{os}\PY{o}{.}\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/home/schirrmr/braindecode/code/braindecode/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \section{Cropped Decoding}\label{cropped-decoding}

    Now we will use cropped decoding. Cropped decoding means the ConvNet is
trained on time windows/time crops within the trials. Most of the code
is identical to the \href{TrialWise_Decoding.html}{Trialwise Decoding
Tutorial}, differences are explained in the text.

    \subsection{Load data}\label{load-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{mne}
        \PY{k+kn}{from} \PY{n+nn}{mne}\PY{n+nn}{.}\PY{n+nn}{io} \PY{k}{import} \PY{n}{concatenate\PYZus{}raws}
        
        \PY{c+c1}{\PYZsh{} 5,6,7,10,13,14 are codes for executed and imagined hands/feet}
        \PY{n}{subject\PYZus{}id} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{event\PYZus{}codes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{13}\PY{p}{,}\PY{l+m+mi}{14}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} This will download the files if you don\PYZsq{}t have them yet,}
        \PY{c+c1}{\PYZsh{} and then return the paths to the files.}
        \PY{n}{physionet\PYZus{}paths} \PY{o}{=} \PY{n}{mne}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{eegbci}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{n}{subject\PYZus{}id}\PY{p}{,} \PY{n}{event\PYZus{}codes}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Load each of the files}
        \PY{n}{parts} \PY{o}{=} \PY{p}{[}\PY{n}{mne}\PY{o}{.}\PY{n}{io}\PY{o}{.}\PY{n}{read\PYZus{}raw\PYZus{}edf}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{n}{preload}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{stim\PYZus{}channel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WARNING}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{k}{for} \PY{n}{path} \PY{o+ow}{in} \PY{n}{physionet\PYZus{}paths}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Concatenate them}
        \PY{n}{raw} \PY{o}{=} \PY{n}{concatenate\PYZus{}raws}\PY{p}{(}\PY{n}{parts}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Find the events in this dataset}
        \PY{n}{events} \PY{o}{=} \PY{n}{mne}\PY{o}{.}\PY{n}{find\PYZus{}events}\PY{p}{(}\PY{n}{raw}\PY{p}{,} \PY{n}{shortest\PYZus{}event}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{stim\PYZus{}channel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{STI 014}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Use only EEG channels}
        \PY{n}{eeg\PYZus{}channel\PYZus{}inds} \PY{o}{=} \PY{n}{mne}\PY{o}{.}\PY{n}{pick\PYZus{}types}\PY{p}{(}\PY{n}{raw}\PY{o}{.}\PY{n}{info}\PY{p}{,} \PY{n}{meg}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{eeg}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{stim}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{eog}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                           \PY{n}{exclude}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bads}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Extract trials, only using EEG channels}
        \PY{n}{epoched} \PY{o}{=} \PY{n}{mne}\PY{o}{.}\PY{n}{Epochs}\PY{p}{(}\PY{n}{raw}\PY{p}{,} \PY{n}{events}\PY{p}{,} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{hands}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{feet}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{tmin}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{tmax}\PY{o}{=}\PY{l+m+mf}{4.1}\PY{p}{,} \PY{n}{proj}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{picks}\PY{o}{=}\PY{n}{eeg\PYZus{}channel\PYZus{}inds}\PY{p}{,}
                        \PY{n}{baseline}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{preload}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Trigger channel has a non-zero initial value of 1 (consider using initial\_event=True to detect this event)
Removing orphaned offset at the beginning of the file.
179 events found
Event IDs: [1 2 3]
90 matching events found
No baseline correction applied
Not setting metadata
Loading data for 90 events and 497 original time points {\ldots}
0 bad epochs dropped

    \end{Verbatim}

    \subsection{Convert data to Braindecode
format}\label{convert-data-to-braindecode-format}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{datautil}\PY{n+nn}{.}\PY{n+nn}{signal\PYZus{}target} \PY{k}{import} \PY{n}{SignalAndTarget}
        \PY{c+c1}{\PYZsh{} Convert data from volt to millivolt}
        \PY{c+c1}{\PYZsh{} Pytorch expects float32 for input and int64 for labels.}
        \PY{n}{X} \PY{o}{=} \PY{p}{(}\PY{n}{epoched}\PY{o}{.}\PY{n}{get\PYZus{}data}\PY{p}{(}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{1e6}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{p}{(}\PY{n}{epoched}\PY{o}{.}\PY{n}{events}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{int64}\PY{p}{)} \PY{c+c1}{\PYZsh{}2,3 \PYZhy{}\PYZgt{} 0,1}
        
        \PY{n}{train\PYZus{}set} \PY{o}{=} \PY{n}{SignalAndTarget}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{60}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{60}\PY{p}{]}\PY{p}{)}
        \PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{SignalAndTarget}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{60}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{p}{[}\PY{l+m+mi}{60}\PY{p}{:}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsection{Create the model}\label{create-the-model}

    For cropped decoding, we now transform the model into a model that
outputs a dense time series of predictions. For this, we manually set
the length of the final convolution layer to some length that makes the
receptive field of the ConvNet smaller than the number of samples in a
trial. Also, we use \texttt{to\_dense\_prediction\_model}, which removes
the strides in the ConvNet and instead uses dilated convolutions to get
a dense output (see \href{https://arxiv.org/abs/1511.07122}{Multi-Scale
Context Aggregation by Dilated Convolutions} and our paper
\href{https://arxiv.org/abs/1703.05051}{Deep learning with convolutional
neural networks for EEG decoding and visualization} Section 2.5.4 for
some background on this).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{braindecode1}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{shallow\PYZus{}fbcsp\PYZus{}filter} \PY{k}{import} \PY{n}{ShallowFBCSPNet}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k+kn}{from} \PY{n+nn}{torch} \PY{k}{import} \PY{n}{nn}
        \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{torch\PYZus{}ext}\PY{n+nn}{.}\PY{n+nn}{util} \PY{k}{import} \PY{n}{set\PYZus{}random\PYZus{}seeds}
        \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{util} \PY{k}{import} \PY{n}{to\PYZus{}dense\PYZus{}prediction\PYZus{}model}
        
        \PY{c+c1}{\PYZsh{} Set if you want to use GPU}
        \PY{c+c1}{\PYZsh{} You can also use torch.cuda.is\PYZus{}available() to determine if cuda is available on your machine.}
        \PY{n}{cuda} \PY{o}{=} \PY{k+kc}{False}
        \PY{n}{set\PYZus{}random\PYZus{}seeds}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{20170629}\PY{p}{,} \PY{n}{cuda}\PY{o}{=}\PY{n}{cuda}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} This will determine how many crops are processed in parallel}
        \PY{n}{input\PYZus{}time\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{450}
        \PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{l+m+mi}{2}
        \PY{n}{in\PYZus{}chans} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} final\PYZus{}conv\PYZus{}length determines the size of the receptive field of the ConvNet}
        \PY{n}{model} \PY{o}{=} \PY{n}{ShallowFBCSPNet}\PY{p}{(}\PY{n}{in\PYZus{}chans}\PY{o}{=}\PY{n}{in\PYZus{}chans}\PY{p}{,} \PY{n}{n\PYZus{}classes}\PY{o}{=}\PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{input\PYZus{}time\PYZus{}length}\PY{o}{=}\PY{n}{input\PYZus{}time\PYZus{}length}\PY{p}{,}
                                \PY{n}{final\PYZus{}conv\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}\PY{o}{.}\PY{n}{create\PYZus{}network}\PY{p}{(}\PY{p}{)}
        \PY{n}{to\PYZus{}dense\PYZus{}prediction\PYZus{}model}\PY{p}{(}\PY{n}{model}\PY{p}{)}
        
        \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
            \PY{n}{model}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
        
        \PY{k+kn}{from} \PY{n+nn}{torch} \PY{k}{import} \PY{n}{optim}
        
        \PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:89: UserWarning: nn.init.xavier\_uniform is now deprecated in favor of nn.init.xavier\_uniform\_.
  init.xavier\_uniform(model.conv\_time.weight, gain=1)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:92: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant\_.
  init.constant(model.conv\_time.bias, 0)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:94: UserWarning: nn.init.xavier\_uniform is now deprecated in favor of nn.init.xavier\_uniform\_.
  init.xavier\_uniform(model.conv\_spat.weight, gain=1)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:98: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant\_.
  init.constant(model.bnorm.weight, 1)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:99: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant\_.
  init.constant(model.bnorm.bias, 0)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:100: UserWarning: nn.init.xavier\_uniform is now deprecated in favor of nn.init.xavier\_uniform\_.
  init.xavier\_uniform(model.conv\_classifier.weight, gain=1)
/home/dansy/Documents/braindecode-master/braindecode1/models/shallow\_fbcsp\_filter.py:101: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant\_.
  init.constant(model.conv\_classifier.bias, 0)

    \end{Verbatim}

    \subsection{Create cropped iterator}\label{create-cropped-iterator}

    For extracting crops from the trials, Braindecode provides the
\texttt{CropsFromTrialsIterator?} class. This class needs to know the
input time length of the inputs you put into the network and the number
of predictions that the ConvNet will output per input. You can determine
the number of predictions by passing dummy data through the ConvNet:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{torch\PYZus{}ext}\PY{n+nn}{.}\PY{n+nn}{util} \PY{k}{import} \PY{n}{np\PYZus{}to\PYZus{}var}
        \PY{c+c1}{\PYZsh{} determine output size}
        \PY{n}{test\PYZus{}input} \PY{o}{=} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{in\PYZus{}chans}\PY{p}{,} \PY{n}{input\PYZus{}time\PYZus{}length}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
            \PY{n}{test\PYZus{}input} \PY{o}{=} \PY{n}{test\PYZus{}input}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
        \PY{n}{out} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{)}
        \PY{n}{n\PYZus{}preds\PYZus{}per\PYZus{}input} \PY{o}{=} \PY{n}{out}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}:d\PYZcb{}}\PY{l+s+s2}{ predictions per input/trial}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}preds\PYZus{}per\PYZus{}input}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
187 predictions per input/trial

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/dansy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for log\_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{datautil}\PY{n+nn}{.}\PY{n+nn}{iterators} \PY{k}{import} \PY{n}{CropsFromTrialsIterator}
        \PY{n}{iterator} \PY{o}{=} \PY{n}{CropsFromTrialsIterator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{input\PYZus{}time\PYZus{}length}\PY{o}{=}\PY{n}{input\PYZus{}time\PYZus{}length}\PY{p}{,}
                                          \PY{n}{n\PYZus{}preds\PYZus{}per\PYZus{}input}\PY{o}{=}\PY{n}{n\PYZus{}preds\PYZus{}per\PYZus{}input}\PY{p}{)}
\end{Verbatim}


    The iterator has the method \texttt{get\_batches}, which can be used to
get randomly shuffled training batches with \texttt{shuffle=True} or
ordered batches (i.e. first from trial 1, then from trial 2, etc.) with
\texttt{shuffle=False}. Additionally, Braindecode provides the
\texttt{compute\_preds\_per\_trial\_for\_set} method, which accepts
predictions from the ordered batches and returns predictions per trial.
It removes any overlapping predictions, which occur if the number of
predictions per input is not a divisor of the number of samples in a
trial.

These methods can also work with trials of different lengths! For
different-length trials, set \texttt{X} to be a list of 2d-arrays
instead of a 3d-array.

    \subsection{Training loop}\label{training-loop}

    The code below uses both the cropped iterator and the
\texttt{compute\_preds\_per\_trial\_for\_set} function to train and
evaluate the network.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{torch\PYZus{}ext}\PY{n+nn}{.}\PY{n+nn}{util} \PY{k}{import} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{,} \PY{n}{var\PYZus{}to\PYZus{}np}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
        \PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{random} \PY{k}{import} \PY{n}{RandomState}
        \PY{k+kn}{import} \PY{n+nn}{torch} \PY{k}{as} \PY{n+nn}{th}
        \PY{k+kn}{from} \PY{n+nn}{braindecode}\PY{n+nn}{.}\PY{n+nn}{experiments}\PY{n+nn}{.}\PY{n+nn}{monitors} \PY{k}{import} \PY{n}{compute\PYZus{}preds\PYZus{}per\PYZus{}trial\PYZus{}for\PYZus{}set}
        \PY{n}{rng} \PY{o}{=} \PY{n}{RandomState}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2017}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{)}
        \PY{k}{for} \PY{n}{i\PYZus{}epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Set model to training mode}
            \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
            \PY{k}{for} \PY{n}{batch\PYZus{}X}\PY{p}{,} \PY{n}{batch\PYZus{}y} \PY{o+ow}{in} \PY{n}{iterator}\PY{o}{.}\PY{n}{get\PYZus{}batches}\PY{p}{(}\PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
                \PY{n}{net\PYZus{}in} \PY{o}{=} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{(}\PY{n}{batch\PYZus{}X}\PY{p}{)}
                \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
                    \PY{n}{net\PYZus{}in} \PY{o}{=} \PY{n}{net\PYZus{}in}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
                \PY{n}{net\PYZus{}target} \PY{o}{=} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{(}\PY{n}{batch\PYZus{}y}\PY{p}{)}
                \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
                    \PY{n}{net\PYZus{}target} \PY{o}{=} \PY{n}{net\PYZus{}target}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Remove gradients of last backward pass from all parameters }
                \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
                \PY{n}{outputs} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{net\PYZus{}in}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Mean predictions across trial}
                \PY{c+c1}{\PYZsh{} Note that this will give identical gradients to computing}
                \PY{c+c1}{\PYZsh{} a per\PYZhy{}prediction loss (at least for the combination of log softmax activation }
                \PY{c+c1}{\PYZsh{} and negative log likelihood loss which we are using here)}
                \PY{n}{outputs} \PY{o}{=} \PY{n}{th}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{keepdim}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{nll\PYZus{}loss}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{net\PYZus{}target}\PY{p}{)}
                \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Print some statistics each epoch}
            \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch }\PY{l+s+si}{\PYZob{}:d\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}epoch}\PY{p}{)}\PY{p}{)}
            \PY{k}{for} \PY{n}{setname}\PY{p}{,} \PY{n}{dataset} \PY{o+ow}{in} \PY{p}{(}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}set}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{test\PYZus{}set}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Collect all predictions and losses}
                \PY{n}{all\PYZus{}preds} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{n}{all\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{n}{batch\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{batch\PYZus{}X}\PY{p}{,} \PY{n}{batch\PYZus{}y} \PY{o+ow}{in} \PY{n}{iterator}\PY{o}{.}\PY{n}{get\PYZus{}batches}\PY{p}{(}\PY{n}{dataset}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
                    \PY{n}{net\PYZus{}in} \PY{o}{=} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{(}\PY{n}{batch\PYZus{}X}\PY{p}{)}
                    \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
                        \PY{n}{net\PYZus{}in} \PY{o}{=} \PY{n}{net\PYZus{}in}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
                    \PY{n}{net\PYZus{}target} \PY{o}{=} \PY{n}{np\PYZus{}to\PYZus{}var}\PY{p}{(}\PY{n}{batch\PYZus{}y}\PY{p}{)}
                    \PY{k}{if} \PY{n}{cuda}\PY{p}{:}
                        \PY{n}{net\PYZus{}target} \PY{o}{=} \PY{n}{net\PYZus{}target}\PY{o}{.}\PY{n}{cuda}\PY{p}{(}\PY{p}{)}
                    \PY{n}{outputs} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{net\PYZus{}in}\PY{p}{)}
                    \PY{n}{all\PYZus{}preds}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{var\PYZus{}to\PYZus{}np}\PY{p}{(}\PY{n}{outputs}\PY{p}{)}\PY{p}{)}
                    \PY{n}{outputs} \PY{o}{=} \PY{n}{th}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{keepdim}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                    \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{nll\PYZus{}loss}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{net\PYZus{}target}\PY{p}{)}
                    \PY{n}{loss} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n}{var\PYZus{}to\PYZus{}np}\PY{p}{(}\PY{n}{loss}\PY{p}{)}\PY{p}{)}
                    \PY{n}{all\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
                    \PY{n}{batch\PYZus{}sizes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{batch\PYZus{}X}\PY{p}{)}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Compute mean per\PYZhy{}input loss }
                \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{all\PYZus{}losses}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{batch\PYZus{}sizes}\PY{p}{)} \PY{o}{/}
                               \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{batch\PYZus{}sizes}\PY{p}{)}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}:6s\PYZcb{}}\PY{l+s+s2}{ Loss: }\PY{l+s+si}{\PYZob{}:.5f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{setname}\PY{p}{,} \PY{n}{loss}\PY{p}{)}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Assign the predictions to the trials}
                \PY{n}{preds\PYZus{}per\PYZus{}trial} \PY{o}{=} \PY{n}{compute\PYZus{}preds\PYZus{}per\PYZus{}trial\PYZus{}for\PYZus{}set}\PY{p}{(}\PY{n}{all\PYZus{}preds}\PY{p}{,}
                                                                  \PY{n}{input\PYZus{}time\PYZus{}length}\PY{p}{,}
                                                                  \PY{n}{dataset}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} preds per trial are now trials x classes x timesteps/predictions}
                \PY{c+c1}{\PYZsh{} Now mean across timesteps for each trial to get per\PYZhy{}trial predictions}
                \PY{n}{meaned\PYZus{}preds\PYZus{}per\PYZus{}trial} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{preds\PYZus{}per\PYZus{}trial}\PY{p}{]}\PY{p}{)}
                \PY{n}{predicted\PYZus{}labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{meaned\PYZus{}preds\PYZus{}per\PYZus{}trial}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{predicted\PYZus{}labels} \PY{o}{==} \PY{n}{dataset}\PY{o}{.}\PY{n}{y}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}:6s\PYZcb{}}\PY{l+s+s2}{ Accuracy: }\PY{l+s+si}{\PYZob{}:.1f\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                    \PY{n}{setname}\PY{p}{,} \PY{n}{accuracy} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/dansy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for log\_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 0
Train  Loss: 0.78555
Train  Accuracy: 65.0\%
Test   Loss: 0.94014
Test   Accuracy: 40.0\%
Epoch 1
Train  Loss: 0.61988
Train  Accuracy: 75.0\%
Test   Loss: 0.85602
Test   Accuracy: 53.3\%
Epoch 2
Train  Loss: 0.52456
Train  Accuracy: 86.7\%
Test   Loss: 0.74200
Test   Accuracy: 53.3\%
Epoch 3
Train  Loss: 0.47379
Train  Accuracy: 95.0\%
Test   Loss: 0.69200
Test   Accuracy: 56.7\%
Epoch 4
Train  Loss: 0.41871
Train  Accuracy: 100.0\%
Test   Loss: 0.65381
Test   Accuracy: 60.0\%
Epoch 5
Train  Loss: 0.33951
Train  Accuracy: 100.0\%
Test   Loss: 0.59527
Test   Accuracy: 73.3\%
Epoch 6
Train  Loss: 0.27745
Train  Accuracy: 98.3\%
Test   Loss: 0.55524
Test   Accuracy: 76.7\%
Epoch 7
Train  Loss: 0.22620
Train  Accuracy: 100.0\%
Test   Loss: 0.54172
Test   Accuracy: 73.3\%
Epoch 8
Train  Loss: 0.19159
Train  Accuracy: 98.3\%
Test   Loss: 0.52840
Test   Accuracy: 73.3\%
Epoch 9
Train  Loss: 0.15539
Train  Accuracy: 100.0\%
Test   Loss: 0.56404
Test   Accuracy: 70.0\%
Epoch 10
Train  Loss: 0.18283
Train  Accuracy: 96.7\%
Test   Loss: 0.54720
Test   Accuracy: 76.7\%
Epoch 11
Train  Loss: 0.15494
Train  Accuracy: 98.3\%
Test   Loss: 0.56017
Test   Accuracy: 76.7\%
Epoch 12
Train  Loss: 0.13952
Train  Accuracy: 98.3\%
Test   Loss: 0.54206
Test   Accuracy: 70.0\%
Epoch 13
Train  Loss: 0.19148
Train  Accuracy: 93.3\%
Test   Loss: 0.64957
Test   Accuracy: 73.3\%
Epoch 14
Train  Loss: 0.13565
Train  Accuracy: 98.3\%
Test   Loss: 0.59017
Test   Accuracy: 73.3\%
Epoch 15
Train  Loss: 0.10701
Train  Accuracy: 100.0\%
Test   Loss: 0.60196
Test   Accuracy: 73.3\%
Epoch 16
Train  Loss: 0.09559
Train  Accuracy: 98.3\%
Test   Loss: 0.62551
Test   Accuracy: 73.3\%
Epoch 17
Train  Loss: 0.07717
Train  Accuracy: 100.0\%
Test   Loss: 0.61589
Test   Accuracy: 70.0\%
Epoch 18
Train  Loss: 0.07380
Train  Accuracy: 98.3\%
Test   Loss: 0.62587
Test   Accuracy: 73.3\%
Epoch 19
Train  Loss: 0.05661
Train  Accuracy: 100.0\%
Test   Loss: 0.62390
Test   Accuracy: 70.0\%

    \end{Verbatim}

    The early stopping leds to have an accuracy of 70.0\% at the epoch 16

    Eventually, we arrive at 76.7\% accuracy, so 23 from 30 trials are
correctly predicted, 4 more than for the trialwise decoding method.

    \subsection{Dataset References}\label{dataset-references}

    This dataset was created and contributed to PhysioNet by the developers
of the \href{http://www.schalklab.org/research/bci2000}{BCI2000}
instrumentation system, which they used in making these recordings. The
system is described in:

\begin{verbatim}
 Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. (2004) BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE TBME 51(6):1034-1043.
\end{verbatim}

\href{https://physionet.org/physiobank/}{PhysioBank} is a large and
growing archive of well-characterized digital recordings of physiologic
signals and related data for use by the biomedical research community
and further described in:

\begin{verbatim}
Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000) PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220.
\end{verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
