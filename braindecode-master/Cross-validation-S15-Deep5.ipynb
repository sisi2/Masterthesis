{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.sys.path.insert(0, '/home/ai/Documents/braindecode-master/braindecode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropped Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use cropped decoding. Cropped decoding means the ConvNet is trained on time windows/time crops within the trials. Most of the code is identical to the [Trialwise Decoding Tutorial](TrialWise_Decoding.html), differences are explained in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger channel has a non-zero initial value of 1 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2 3]\n",
      "90 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "Loading data for 90 events and 497 original time points ...\n",
      "6 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from mne.io import concatenate_raws\n",
    "\n",
    "# 5,6,7,10,13,14 are codes for executed and imagined hands/feet\n",
    "subject_id = 15\n",
    "event_codes = [5,6,9,10,13,14]\n",
    "\n",
    "# This will download the files if you don't have them yet,\n",
    "# and then return the paths to the files.\n",
    "physionet_paths = mne.datasets.eegbci.load_data(subject_id, event_codes)\n",
    "\n",
    "# Load each of the files\n",
    "parts = [mne.io.read_raw_edf(path, preload=True,stim_channel='auto', verbose='WARNING')\n",
    "         for path in physionet_paths]\n",
    "\n",
    "# Concatenate them\n",
    "raw = concatenate_raws(parts)\n",
    "\n",
    "# Find the events in this dataset\n",
    "events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "# Use only EEG channels\n",
    "eeg_channel_inds = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                   exclude='bads')\n",
    "\n",
    "# Extract trials, only using EEG channels\n",
    "epoched = mne.Epochs(raw, events, dict(hands=2, feet=3), tmin=1, tmax=4.1, proj=False, picks=eeg_channel_inds,\n",
    "                baseline=None, preload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to Braindecode format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 64, 497)\n",
      "(84,)\n",
      "<braindecode.datautil.signal_target.SignalAndTarget object at 0x7fb7b51b61d0>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "# Convert data from volt to millivolt\n",
    "# Pytorch expects float32 for input and int64 for labels.\n",
    "#print(events[:,1])\n",
    "#print(events[:,2]-2)\n",
    "#print(epoched.events[:,2]-2)\n",
    "X = (epoched.get_data() * 1e6).astype(np.float32)\n",
    "print(X.shape)\n",
    "y = (epoched.events[:,2] - 2).astype(np.int64) #2,3 -> 0,1\n",
    "print(y.shape)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = X[:60],X[60:],y[:60],y[60:]\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "data = SignalAndTarget(X , y)\n",
    "train_set = SignalAndTarget(X[:60], y=y[:60])\n",
    "test_set = SignalAndTarget(X[60:], y=y[60:])\n",
    "\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cropped decoding, we now transform the model into a model that outputs a dense time series of predictions.\n",
    "For this, we manually set the length of the final convolution layer to some length that makes the receptive field of the ConvNet smaller than the number of samples in a trial. Also, we use `to_dense_prediction_model`, which removes the strides in the ConvNet and instead uses dilated convolutions to get a dense output (see [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) and our paper [Deep learning with convolutional neural networks for EEG decoding and visualization](https://arxiv.org/abs/1703.05051) Section 2.5.4 for some background on this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Sequential(\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d(1, 25, kernel_size=(10, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d(25, 25, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (conv_nonlin): Expression(expression=elu)\n",
      "  (pool): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(2, 1), ceil_mode=False)\n",
      "  (pool_nonlin): Expression(expression=identity)\n",
      "  (drop_2): Dropout(p=0.5)\n",
      "  (conv_2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), dilation=(2, 1), bias=False)\n",
      "  (bnorm_2): BatchNorm2d(50, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (nonlin_2): Expression(expression=elu)\n",
      "  (pool_2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(4, 1), ceil_mode=False)\n",
      "  (pool_nonlin_2): Expression(expression=identity)\n",
      "  (drop_3): Dropout(p=0.5)\n",
      "  (conv_3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), dilation=(4, 1), bias=False)\n",
      "  (bnorm_3): BatchNorm2d(100, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (nonlin_3): Expression(expression=elu)\n",
      "  (pool_3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(8, 1), ceil_mode=False)\n",
      "  (pool_nonlin_3): Expression(expression=identity)\n",
      "  (drop_4): Dropout(p=0.5)\n",
      "  (conv_4): Conv2d(100, 150, kernel_size=(10, 1), stride=(1, 1), dilation=(8, 1), bias=False)\n",
      "  (bnorm_4): BatchNorm2d(150, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (nonlin_4): Expression(expression=elu)\n",
      "  (pool_4): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(16, 1), ceil_mode=False)\n",
      "  (pool_nonlin_4): Expression(expression=identity)\n",
      "  (drop_5): Dropout(p=0.5)\n",
      "  (conv_5): Conv2d(150, 200, kernel_size=(10, 1), stride=(1, 1), dilation=(16, 1), bias=False)\n",
      "  (bnorm_5): BatchNorm2d(200, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (nonlin_5): Expression(expression=elu)\n",
      "  (pool_5): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(32, 1), ceil_mode=False)\n",
      "  (pool_nonlin_5): Expression(expression=identity)\n",
      "  (conv_classifier): Conv2d(200, 2, kernel_size=(1, 1), stride=(1, 1), dilation=(32, 1))\n",
      "  (softmax): LogSoftmax()\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:159: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(model.conv_time.weight, gain=1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:162: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(model.conv_time.bias, 0)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:164: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(model.conv_spat.weight, gain=1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:168: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(model.bnorm.weight, 1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:169: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(model.bnorm.bias, 0)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:173: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(conv_weight, gain=1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:180: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(bnorm_weight, 1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:181: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(bnorm_bias, 0)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:183: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(model.conv_classifier.weight, gain=1)\n",
      "/home/dansy/Documents/braindecode-master/braindecode1/models/deep5.py:184: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(model.conv_classifier.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "from braindecode1.models.shallow_fbcsp_conv2 import ShallowFBCSPNet\n",
    "from braindecode1.models.deep5 import Deep4Net\n",
    "\n",
    "#from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = False\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 450\n",
    "\n",
    "n_classes = 2\n",
    "print(train_set.X.shape[1])\n",
    "in_chans = train_set.X.shape[1]\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model =  Deep4Net(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length, stride_before_pool=True,\n",
    "                        pool_time_stride=2,final_conv_length=1).create_network()\n",
    "\n",
    "to_dense_prediction_model(model)\n",
    "print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cropped iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting crops from the trials, Braindecode provides the  `CropsFromTrialsIterator?` class. This class needs to know the input time length of the inputs you put into the network and the number of predictions that the ConvNet will output per input. You can determine the number of predictions by passing dummy data through the ConvNet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 450, 1])\n",
      "47 predictions per input/trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dansy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "print(test_input.shape)\n",
    "out = model(test_input)\n",
    "\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object CropsFromTrialsIterator._yield_block_batches at 0x7fb7832ed830>\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=64,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "print(iterator.get_batches(train_set, shuffle=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator has the method `get_batches`, which can be used to get randomly shuffled training batches with `shuffle=True` or ordered batches (i.e. first from trial 1, then from trial 2, etc.) with `shuffle=False`. Additionally, Braindecode provides the `compute_preds_per_trial_for_set` method, which accepts predictions from the ordered batches and returns predictions per trial. It removes any overlapping predictions, which occur if the number of predictions per input is not a divisor of the number of samples in a trial.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "These methods can also work with trials of different lengths! For different-length trials, set `X` to be a list of 2d-arrays instead of a 3d-array.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses both the cropped iterator and the `compute_preds_per_trial_for_set` function to train and evaluate the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold for cross validation\n",
    "from sklearn.datasets import make_classification\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "KFold(n_splits=5, random_state=7, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#kf = KFold(n_splits=6) # Define the split - into 3 folds \n",
    "kf = KFold(n_splits=5, random_state=7) #evaluate the k fold cross validation\n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf.get_n_splits(X)) \n",
    "print(kf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n",
      " 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n",
      " 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 34 35 36 37 38 39 40\n",
      " 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n",
      " 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83] TEST: [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n",
      " 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83] TEST: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83] TEST: [51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67] TEST: [68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 64)\n",
      "(16, 64)\n",
      "(68,)\n",
      "68 68\n"
     ]
    }
   ],
   "source": [
    "# reshaping the 3D array of the X dataset into a 2D array dataset for predictions \n",
    "                                                    #computation usage\n",
    "Xtrain=X_train[:,:,496:]\n",
    "Xtrain=np.squeeze(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "\n",
    "Xtest=X_test[:,:,496:]\n",
    "Xtest=np.squeeze(Xtest)\n",
    "print(Xtest.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(len(y_train), len(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dansy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.DataFrame(np.squeeze(X[:,:,496:])) # reshaping the 3D array of the X dataset into a 2D array dataset for predictions \n",
    "Xdata =  np_to_var(np.squeeze(X[:,:,496:]))                                                  #computation usage\n",
    "print(len(df))\n",
    "# fit a model\n",
    "#lm = linear_model.LinearRegression()\n",
    "\n",
    "#model = lm.fit(Xtrain, y_train)\n",
    "#predictions = lm.predict(Xtest)\n",
    "#print(Xtrain.shape, y_train.shape, Xtest.shape)\n",
    "\n",
    "#Kfolds Cross validation\n",
    "for batch_X, batch_y in iterator.get_batches(train_set, shuffle=False):\n",
    "    #pass\n",
    "    for train_index, test_index in kf.split(batch_X):\n",
    "       # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = batch_X[train_index], batch_X[test_index]\n",
    "        y_train, y_test = batch_y[train_index], batch_y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dansy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train  Loss: 4.57662\n",
      "Train  Accuracy: 50.0%\n",
      "Test   Loss: 4.43167\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 2\n",
      "Train  Loss: 1.61903\n",
      "Train  Accuracy: 50.0%\n",
      "Test   Loss: 1.10399\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 3\n",
      "Train  Loss: 1.63611\n",
      "Train  Accuracy: 52.1%\n",
      "Test   Loss: 1.14656\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 3.94262\n",
      "Train  Accuracy: 52.1%\n",
      "Test   Loss: 3.57985\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 5\n",
      "Train  Loss: 3.79005\n",
      "Train  Accuracy: 50.0%\n",
      "Test   Loss: 4.03226\n",
      "Test   Accuracy: 50.0%\n",
      "Train  Mean fold Accuracy: 50.8%\n",
      "Train  Mean fold Loss: 3.1%\n",
      "Test   Mean fold Accuracy: 51.7%\n",
      "Test   Mean fold Loss: 2.9%\n",
      "Epoch 1\n",
      "Fold 1\n",
      "Train  Loss: 3.02160\n",
      "Train  Accuracy: 54.2%\n",
      "Test   Loss: 3.97821\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 2\n",
      "Train  Loss: 2.13492\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 3.47637\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.73603\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 2.91049\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 2.11266\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 3.36514\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 2.35264\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 3.76279\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 55.4%\n",
      "Train  Mean fold Loss: 2.7%\n",
      "Test   Mean fold Accuracy: 54.2%\n",
      "Test   Mean fold Loss: 3.2%\n",
      "Epoch 2\n",
      "Fold 1\n",
      "Train  Loss: 2.40830\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 4.03243\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 2.04900\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 3.77860\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 2.02029\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 3.98032\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.86850\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.04144\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.82469\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.18796\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 57.6%\n",
      "Train  Mean fold Loss: 2.5%\n",
      "Test   Mean fold Accuracy: 55.6%\n",
      "Test   Mean fold Loss: 3.5%\n",
      "Epoch 3\n",
      "Fold 1\n",
      "Train  Loss: 1.95092\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.42078\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 2\n",
      "Train  Loss: 1.98160\n",
      "Train  Accuracy: 58.3%\n",
      "Test   Loss: 4.61279\n",
      "Test   Accuracy: 50.0%\n",
      "Fold 3\n",
      "Train  Loss: 1.79193\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.43004\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.76192\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.33886\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.76846\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.30699\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 58.9%\n",
      "Train  Mean fold Loss: 2.3%\n",
      "Test   Mean fold Accuracy: 55.4%\n",
      "Test   Mean fold Loss: 3.7%\n",
      "Epoch 4\n",
      "Fold 1\n",
      "Train  Loss: 1.82923\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.34806\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.93575\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 4.53834\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.70818\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.29593\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.70681\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.34846\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.70179\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.50733\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 59.5%\n",
      "Train  Mean fold Loss: 2.2%\n",
      "Test   Mean fold Accuracy: 56.0%\n",
      "Test   Mean fold Loss: 3.8%\n",
      "Epoch 5\n",
      "Fold 1\n",
      "Train  Loss: 1.69221\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.60014\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.60435\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.60534\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.66173\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.77573\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.62552\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.87567\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.61045\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.94327\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 60.0%\n",
      "Train  Mean fold Loss: 2.1%\n",
      "Test   Mean fold Accuracy: 56.4%\n",
      "Test   Mean fold Loss: 4.0%\n",
      "Epoch 6\n",
      "Fold 1\n",
      "Train  Loss: 1.58538\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.02610\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.65391\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 5.20872\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.68806\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 5.34333\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.61389\n",
      "Train  Accuracy: 60.4%\n",
      "Test   Loss: 5.35454\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.54628\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.33450\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 60.2%\n",
      "Train  Mean fold Loss: 2.0%\n",
      "Test   Mean fold Accuracy: 56.7%\n",
      "Test   Mean fold Loss: 4.2%\n",
      "Epoch 7\n",
      "Fold 1\n",
      "Train  Loss: 1.46130\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.22639\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.53593\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.37058\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.52628\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.25265\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.45501\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.21221\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.39080\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.13410\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 60.6%\n",
      "Train  Mean fold Loss: 2.0%\n",
      "Test   Mean fold Accuracy: 56.9%\n",
      "Test   Mean fold Loss: 4.3%\n",
      "Epoch 8\n",
      "Fold 1\n",
      "Train  Loss: 1.36309\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.04064\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.40609\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.15848\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.44059\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.18308\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.45538\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.12269\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.37608\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.02135\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 60.8%\n",
      "Train  Mean fold Loss: 1.9%\n",
      "Test   Mean fold Accuracy: 57.0%\n",
      "Test   Mean fold Loss: 4.4%\n",
      "Epoch 9\n",
      "Fold 1\n",
      "Train  Loss: 1.42897\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.08989\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.33714\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.00330\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.28060\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.85382\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.30501\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.92069\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.44310\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.10420\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 61.0%\n",
      "Train  Mean fold Loss: 1.9%\n",
      "Test   Mean fold Accuracy: 57.2%\n",
      "Test   Mean fold Loss: 4.5%\n",
      "Epoch 10\n",
      "Fold 1\n",
      "Train  Loss: 1.16541\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.77809\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.31461\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.92605\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.29005\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.99119\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.25408\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.90286\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.14743\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.73743\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 61.4%\n",
      "Train  Mean fold Loss: 1.8%\n",
      "Test   Mean fold Accuracy: 57.3%\n",
      "Test   Mean fold Loss: 4.5%\n",
      "Epoch 11\n",
      "Fold 1\n",
      "Train  Loss: 1.26942\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 4.86281\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.21935\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.86372\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.20304\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.74656\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.15725\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 4.76125\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.21674\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.83905\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 61.8%\n",
      "Train  Mean fold Loss: 1.7%\n",
      "Test   Mean fold Accuracy: 57.4%\n",
      "Test   Mean fold Loss: 4.5%\n",
      "Epoch 12\n",
      "Fold 1\n",
      "Train  Loss: 1.20214\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.76376\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.18699\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.71191\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.22716\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.75261\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.15228\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.63617\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.20528\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.75553\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 62.1%\n",
      "Train  Mean fold Loss: 1.7%\n",
      "Test   Mean fold Accuracy: 57.4%\n",
      "Test   Mean fold Loss: 4.5%\n",
      "Epoch 13\n",
      "Fold 1\n",
      "Train  Loss: 1.13891\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.67004\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.26911\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.84809\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.21981\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.82338\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.29218\n",
      "Train  Accuracy: 64.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test   Loss: 4.96978\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.27851\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.95518\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 62.3%\n",
      "Train  Mean fold Loss: 1.7%\n",
      "Test   Mean fold Accuracy: 57.5%\n",
      "Test   Mean fold Loss: 4.6%\n",
      "Epoch 14\n",
      "Fold 1\n",
      "Train  Loss: 1.30530\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.04727\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.23979\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.03315\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.25386\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.10745\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.19431\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.08070\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.23065\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.13110\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 62.5%\n",
      "Train  Mean fold Loss: 1.6%\n",
      "Test   Mean fold Accuracy: 57.6%\n",
      "Test   Mean fold Loss: 4.6%\n",
      "Epoch 15\n",
      "Fold 1\n",
      "Train  Loss: 1.21009\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.16089\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.27420\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.25306\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.16735\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.14309\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.15599\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 4.94515\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.19556\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.84269\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 62.8%\n",
      "Train  Mean fold Loss: 1.6%\n",
      "Test   Mean fold Accuracy: 57.6%\n",
      "Test   Mean fold Loss: 4.6%\n",
      "Epoch 16\n",
      "Fold 1\n",
      "Train  Loss: 1.37010\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.88687\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.38090\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.79593\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.35215\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.68819\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.41203\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.74061\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.41148\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 4.81063\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 63.0%\n",
      "Train  Mean fold Loss: 1.6%\n",
      "Test   Mean fold Accuracy: 57.6%\n",
      "Test   Mean fold Loss: 4.6%\n",
      "Epoch 17\n",
      "Fold 1\n",
      "Train  Loss: 1.40751\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 4.84455\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.46047\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.11489\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.31926\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.04285\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.30441\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.21878\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.23216\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.15952\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 63.1%\n",
      "Train  Mean fold Loss: 1.6%\n",
      "Test   Mean fold Accuracy: 57.7%\n",
      "Test   Mean fold Loss: 4.7%\n",
      "Epoch 18\n",
      "Fold 1\n",
      "Train  Loss: 1.27516\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.35048\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.18066\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.23595\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.14145\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.24833\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.14436\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.26517\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.15849\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.27839\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 63.4%\n",
      "Train  Mean fold Loss: 1.6%\n",
      "Test   Mean fold Accuracy: 57.7%\n",
      "Test   Mean fold Loss: 4.7%\n",
      "Epoch 19\n",
      "Fold 1\n",
      "Train  Loss: 1.18947\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.39084\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.17807\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.34767\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.23908\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.41066\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.20549\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.46015\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.13855\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.34981\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 63.7%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.8%\n",
      "Test   Mean fold Loss: 4.7%\n",
      "Epoch 20\n",
      "Fold 1\n",
      "Train  Loss: 1.10200\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.14005\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.12410\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.06014\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.14590\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.08835\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.21012\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.11514\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.21901\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.09979\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 63.9%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.8%\n",
      "Test   Mean fold Loss: 4.7%\n",
      "Epoch 21\n",
      "Fold 1\n",
      "Train  Loss: 1.10963\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.00735\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.19273\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.07905\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.21465\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.15963\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.15576\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.11580\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.17286\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.12527\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.0%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.8%\n",
      "Test   Mean fold Loss: 4.8%\n",
      "Epoch 22\n",
      "Fold 1\n",
      "Train  Loss: 1.18755\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.21974\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.15339\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.15450\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.18369\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.22592\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.19121\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.15695\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.28180\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.26874\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.1%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.8%\n",
      "Test   Mean fold Loss: 4.8%\n",
      "Epoch 23\n",
      "Fold 1\n",
      "Train  Loss: 1.23382\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.24784\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.27499\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.24810\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.21744\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.17296\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.27260\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.28714\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.26435\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.28123\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.1%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.8%\n",
      "Test   Mean fold Loss: 4.8%\n",
      "Epoch 24\n",
      "Fold 1\n",
      "Train  Loss: 1.36938\n",
      "Train  Accuracy: 62.5%\n",
      "Test   Loss: 5.47588\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.19185\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.34050\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.27687\n",
      "Train  Accuracy: 64.6%\n",
      "Test   Loss: 5.46421\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.20850\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.38262\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.26295\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.47101\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.2%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.8%\n",
      "Epoch 25\n",
      "Fold 1\n",
      "Train  Loss: 1.12337\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.40782\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.17594\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.46139\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.11072\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.39390\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.08272\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.41763\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.08509\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.45604\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.3%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.8%\n",
      "Epoch 26\n",
      "Fold 1\n",
      "Train  Loss: 1.16808\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.53396\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.10182\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.46118\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.10628\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.37056\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.09624\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.37401\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.08666\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.39464\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.5%\n",
      "Train  Mean fold Loss: 1.5%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 27\n",
      "Fold 1\n",
      "Train  Loss: 1.03044\n",
      "Train  Accuracy: 72.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test   Loss: 5.31165\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.16500\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.42384\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.13452\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.32732\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.02084\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.17897\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.07395\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.24084\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.7%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 28\n",
      "Fold 1\n",
      "Train  Loss: 1.02204\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.16805\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.07725\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.26251\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.05491\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.23603\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.15161\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.37222\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.15176\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.36118\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 64.9%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 29\n",
      "Fold 1\n",
      "Train  Loss: 1.08534\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.25167\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.04188\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.25690\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.06539\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.31659\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.10317\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.34017\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.03512\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.24687\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 65.1%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 57.9%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 30\n",
      "Fold 1\n",
      "Train  Loss: 1.01479\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.20980\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.07145\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.36490\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.07185\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.31606\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.05005\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.30518\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.17646\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.43894\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 65.3%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 31\n",
      "Fold 1\n",
      "Train  Loss: 1.01616\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.19765\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.10965\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.42188\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.02267\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.27573\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.03799\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.27050\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.94934\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.19101\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 65.6%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 32\n",
      "Fold 1\n",
      "Train  Loss: 1.12890\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.36450\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.03804\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.30938\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.08240\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.35135\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.07354\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.33006\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.06698\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.43069\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 65.8%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 4.9%\n",
      "Epoch 33\n",
      "Fold 1\n",
      "Train  Loss: 1.21601\n",
      "Train  Accuracy: 66.7%\n",
      "Test   Loss: 5.55975\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.14874\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.46805\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.11637\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.47731\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.09185\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.48943\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.04316\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.40553\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 65.9%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 34\n",
      "Fold 1\n",
      "Train  Loss: 1.18142\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.51577\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.02480\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.37155\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.96874\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.45903\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.06561\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.45861\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.10943\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.54339\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 66.1%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 35\n",
      "Fold 1\n",
      "Train  Loss: 1.08961\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.50424\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.10453\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.60245\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.08126\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.50845\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.02820\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.46695\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.00692\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.46188\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 66.3%\n",
      "Train  Mean fold Loss: 1.4%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 36\n",
      "Fold 1\n",
      "Train  Loss: 1.04136\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.55169\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.03221\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.40743\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.97649\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.46650\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00045\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.45655\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.03743\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.52539\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 66.5%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 37\n",
      "Fold 1\n",
      "Train  Loss: 0.95662\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.46137\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.97090\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.49942\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.11635\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.70415\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.01269\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.52175\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.98024\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.49594\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 66.6%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 38\n",
      "Fold 1\n",
      "Train  Loss: 1.00114\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.59909\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.92511\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.57562\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.98935\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.62509\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.93084\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.48204\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.98883\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.63137\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 66.8%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 39\n",
      "Fold 1\n",
      "Train  Loss: 0.93918\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.56473\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.01249\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.61590\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.94780\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.51849\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.99733\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.67093\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.96325\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.64427\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.0%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.0%\n",
      "Epoch 40\n",
      "Fold 1\n",
      "Train  Loss: 1.01923\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.62017\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.09289\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79225\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.99903\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.73358\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.94149\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.54972\n",
      "Test   Accuracy: 58.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n",
      "Train  Loss: 1.00708\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.61657\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.2%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.0%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 41\n",
      "Fold 1\n",
      "Train  Loss: 1.03533\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.61859\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.09320\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.77087\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.95234\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.50856\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.09002\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.68405\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.01725\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.57210\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.4%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 42\n",
      "Fold 1\n",
      "Train  Loss: 1.09455\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.58812\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.00994\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.50105\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.04847\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.57388\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.92933\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.38482\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.03397\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.61518\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.5%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 43\n",
      "Fold 1\n",
      "Train  Loss: 1.06270\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.58222\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.92175\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.43025\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.98260\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.42574\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00739\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.48589\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.96906\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.39626\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.7%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 44\n",
      "Fold 1\n",
      "Train  Loss: 0.97284\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.47857\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.03035\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.56156\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.94767\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.41244\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00386\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.48936\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.06993\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.63347\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.8%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 45\n",
      "Fold 1\n",
      "Train  Loss: 1.07883\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.75103\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.98355\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.53214\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.06242\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.66093\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.01443\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.64220\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.99052\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.65736\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 67.9%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 46\n",
      "Fold 1\n",
      "Train  Loss: 1.01828\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.74976\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.97918\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.64876\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.12574\n",
      "Train  Accuracy: 68.8%\n",
      "Test   Loss: 5.90947\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.05534\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.78192\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.04855\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.80336\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.0%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 47\n",
      "Fold 1\n",
      "Train  Loss: 1.01738\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.72890\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.92820\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.62482\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.04163\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.79839\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.01405\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.77055\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.06249\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.74910\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.1%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.1%\n",
      "Epoch 48\n",
      "Fold 1\n",
      "Train  Loss: 1.02993\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.83348\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.06050\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.77417\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.00831\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.70777\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00181\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.71414\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.94616\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.61088\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.2%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 49\n",
      "Fold 1\n",
      "Train  Loss: 0.94692\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.68078\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.95647\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.65522\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.00471\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.74698\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.91917\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.61596\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.02854\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.69965\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.2%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 50\n",
      "Fold 1\n",
      "Train  Loss: 0.92969\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.61854\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.00466\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.76025\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.97047\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.76232\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.93942\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.63168\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.95494\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.63407\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.3%\n",
      "Train  Mean fold Loss: 1.3%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 51\n",
      "Fold 1\n",
      "Train  Loss: 0.98144\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.77285\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.98010\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.65799\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.90173\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.63496\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00589\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.74139\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.87992\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.53694\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.4%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 52\n",
      "Fold 1\n",
      "Train  Loss: 0.97108\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.61754\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.97317\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.65505\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.92720\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.61503\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.87226\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.56078\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.89061\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.57172\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.5%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 53\n",
      "Fold 1\n",
      "Train  Loss: 0.85268\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.53560\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.87839\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.50620\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.90849\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.51774\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.85294\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.41755\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.75891\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.35902\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.7%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 54\n",
      "Fold 1\n",
      "Train  Loss: 0.84454\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.45502\n",
      "Test   Accuracy: 58.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "Train  Loss: 0.76345\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.32381\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.77757\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.31645\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.79396\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.43589\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.74820\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.35107\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.8%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 55\n",
      "Fold 1\n",
      "Train  Loss: 0.75226\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.35752\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.80189\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.38065\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.75407\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.32605\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.85763\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.50873\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.88581\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.58039\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 68.9%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 56\n",
      "Fold 1\n",
      "Train  Loss: 0.78273\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.42058\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.78443\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.40366\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.82068\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.48700\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.80830\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.41994\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.84911\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.48872\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.0%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 57\n",
      "Fold 1\n",
      "Train  Loss: 0.86903\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.55877\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.76907\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.42083\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.88662\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.53939\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83734\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.59340\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.79037\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.55076\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.1%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 58\n",
      "Fold 1\n",
      "Train  Loss: 0.81853\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.64118\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.87516\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.67047\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.87949\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78209\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.85852\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.77018\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.84038\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.69346\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.2%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 59\n",
      "Fold 1\n",
      "Train  Loss: 0.92356\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78270\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.86165\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79765\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.87318\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.70574\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.93230\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78895\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.87386\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.73719\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.3%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 60\n",
      "Fold 1\n",
      "Train  Loss: 0.80622\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.69587\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.93019\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79943\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.88591\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.71930\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.92292\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82977\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.82393\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.66751\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.4%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 61\n",
      "Fold 1\n",
      "Train  Loss: 0.93261\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78395\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.82984\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.63954\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.79347\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.60104\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.84496\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75305\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.85999\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.63899\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.5%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.2%\n",
      "Epoch 62\n",
      "Fold 1\n",
      "Train  Loss: 0.87181\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.62080\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.87481\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.64130\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.96733\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79184\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.06233\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.85829\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.01912\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.74653\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.6%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.1%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 63\n",
      "Fold 1\n",
      "Train  Loss: 1.07974\n",
      "Train  Accuracy: 70.8%\n",
      "Test   Loss: 5.84184\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 1.03323\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.81663\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.02477\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.76569\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.92332\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.67098\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.05043\n",
      "Train  Accuracy: 72.9%\n",
      "Test   Loss: 5.77121\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.6%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 64\n",
      "Fold 1\n",
      "Train  Loss: 0.92651\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.61268\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.98886\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.70299\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.01755\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80256\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.00900\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.74438\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.99553\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.65327\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.7%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 65\n",
      "Fold 1\n",
      "Train  Loss: 0.99964\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80890\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.98612\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86988\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.83328\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.74048\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.96141\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.95441\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.97274\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.97540\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.8%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 66\n",
      "Fold 1\n",
      "Train  Loss: 0.96138\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.93383\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.90538\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89778\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.90654\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.96443\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.87012\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85729\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.85583\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92498\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.9%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 67\n",
      "Fold 1\n",
      "Train  Loss: 0.83897\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82637\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.92603\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.97282\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.82380\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86474\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.93375\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.99396\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 1.01098\n",
      "Train  Accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test   Loss: 6.11401\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 69.9%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 68\n",
      "Fold 1\n",
      "Train  Loss: 0.93017\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.97410\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.89655\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.87672\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.94144\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92209\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.90432\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.96554\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.89276\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78759\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.0%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 69\n",
      "Fold 1\n",
      "Train  Loss: 0.88600\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80647\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.91986\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88578\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 1.03302\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 6.03153\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 1.02894\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 6.02486\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.92564\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89134\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.1%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 70\n",
      "Fold 1\n",
      "Train  Loss: 0.91029\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85627\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.92816\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92894\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.93316\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86856\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.92486\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86226\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.89560\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81225\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.2%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 71\n",
      "Fold 1\n",
      "Train  Loss: 0.88875\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85434\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.93954\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81906\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.87945\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82296\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.88138\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75739\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.86129\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75219\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.2%\n",
      "Train  Mean fold Loss: 1.2%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 72\n",
      "Fold 1\n",
      "Train  Loss: 0.95767\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86080\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.96239\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78568\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.89658\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80541\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.92902\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80830\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.91499\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81617\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.3%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 73\n",
      "Fold 1\n",
      "Train  Loss: 0.94758\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81756\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85864\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84553\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.96648\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.94079\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.80890\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.69339\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.90807\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82060\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.3%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.3%\n",
      "Epoch 74\n",
      "Fold 1\n",
      "Train  Loss: 0.94251\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82909\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85808\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75191\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.85766\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.69008\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.94340\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92851\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.95387\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84289\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.4%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 75\n",
      "Fold 1\n",
      "Train  Loss: 0.92984\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86665\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.91767\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.83815\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.81167\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.64819\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.88402\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.76844\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.91814\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79863\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.5%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 76\n",
      "Fold 1\n",
      "Train  Loss: 0.86260\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.73937\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.89729\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75686\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.91154\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81145\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83257\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.68603\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.85515\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.69221\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.5%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 77\n",
      "Fold 1\n",
      "Train  Loss: 0.88002\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.74359\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85416\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.73636\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.78036\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.65651\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.88262\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.70673\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.86332\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85626\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.6%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 78\n",
      "Fold 1\n",
      "Train  Loss: 0.97324\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90037\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.86000\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79363\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.84252\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75135\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.89981\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84940\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.83558\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79564\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.6%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 79\n",
      "Fold 1\n",
      "Train  Loss: 0.84124\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.77952\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85503\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85506\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.83963\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.83216\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.82348\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.87216\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.85860\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.83787\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.7%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 80\n",
      "Fold 1\n",
      "Train  Loss: 0.92702\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.95614\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.81948\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78399\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.83038\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.87479\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.94629\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.96028\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.83710\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90771\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.8%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 81\n",
      "Fold 1\n",
      "Train  Loss: 0.85108\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89594\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85054\n",
      "Train  Accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test   Loss: 5.88508\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.81892\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.83791\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.81178\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89488\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.82734\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.85430\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.8%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 82\n",
      "Fold 1\n",
      "Train  Loss: 0.77262\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.84649\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.88158\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92235\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.85554\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89110\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.82824\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88585\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.77654\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.79149\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.9%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 83\n",
      "Fold 1\n",
      "Train  Loss: 0.84016\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88774\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.86201\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.93074\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.83842\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86774\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.85967\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.97033\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.81476\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.89428\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 70.9%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 84\n",
      "Fold 1\n",
      "Train  Loss: 0.90215\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88315\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.89206\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.91173\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.81376\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78702\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.79901\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.83452\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.92656\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.95529\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.0%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 85\n",
      "Fold 1\n",
      "Train  Loss: 0.92676\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 6.00365\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.82425\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82787\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.86962\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90118\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83788\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88046\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.87935\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.91404\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.0%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 86\n",
      "Fold 1\n",
      "Train  Loss: 0.82918\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79443\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.83268\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.81886\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.72523\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.62932\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.93913\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.94479\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.88992\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89678\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.1%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 87\n",
      "Fold 1\n",
      "Train  Loss: 0.88826\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90981\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.80986\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.79726\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.78423\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.72087\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.79253\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.76842\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.85901\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80530\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.1%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 88\n",
      "Fold 1\n",
      "Train  Loss: 0.87072\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90229\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.89253\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86432\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.81575\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82645\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.88793\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92045\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.84363\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.78461\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.2%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 89\n",
      "Fold 1\n",
      "Train  Loss: 0.85034\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84909\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.87084\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.89083\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.80369\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.80263\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.88135\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.93469\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.93506\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.94249\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.2%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 90\n",
      "Fold 1\n",
      "Train  Loss: 0.81827\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82303\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.77375\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.76363\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.80601\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.75716\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83238\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84096\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.78552\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.76590\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.3%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 91\n",
      "Fold 1\n",
      "Train  Loss: 0.82298\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.82065\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.83131\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.74611\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.83877\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.87937\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83422\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.84380\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.81766\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.88597\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.3%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 92\n",
      "Fold 1\n",
      "Train  Loss: 0.78423\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.80830\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.85212\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.94077\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.78170\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.78803\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.74695\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.86427\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.75118\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.75624\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.4%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 93\n",
      "Fold 1\n",
      "Train  Loss: 0.81264\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.92812\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.77799\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.88171\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.79127\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.89570\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.78030\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.95283\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.75548\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.87761\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.4%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.4%\n",
      "Epoch 94\n",
      "Fold 1\n",
      "Train  Loss: 0.79566\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.82428\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.79458\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.76662\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.73129\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.78189\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.76287\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.83042\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.73815\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.76580\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.5%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Epoch 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train  Loss: 0.77548\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.80649\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.77580\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.82304\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.72772\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.78850\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.80861\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.88138\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.77381\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.83307\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.5%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Epoch 96\n",
      "Fold 1\n",
      "Train  Loss: 0.87419\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.96129\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.71001\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.79922\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.70150\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.71873\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.77725\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.84292\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.73519\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.76652\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.6%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Epoch 97\n",
      "Fold 1\n",
      "Train  Loss: 0.82870\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.95112\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.80308\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.90337\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.76911\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.87413\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.81354\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90361\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.82652\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.99156\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.6%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Epoch 98\n",
      "Fold 1\n",
      "Train  Loss: 0.74164\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.80198\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.83055\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.92538\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.82380\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90658\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.75754\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.84830\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.77792\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.84270\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.7%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Epoch 99\n",
      "Fold 1\n",
      "Train  Loss: 0.78256\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.87524\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 2\n",
      "Train  Loss: 0.83163\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.90699\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 3\n",
      "Train  Loss: 0.78285\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.85368\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 4\n",
      "Train  Loss: 0.83967\n",
      "Train  Accuracy: 75.0%\n",
      "Test   Loss: 5.86759\n",
      "Test   Accuracy: 58.3%\n",
      "Fold 5\n",
      "Train  Loss: 0.74951\n",
      "Train  Accuracy: 77.1%\n",
      "Test   Loss: 5.79137\n",
      "Test   Accuracy: 58.3%\n",
      "Train  Mean fold Accuracy: 71.7%\n",
      "Train  Mean fold Loss: 1.1%\n",
      "Test   Mean fold Accuracy: 58.2%\n",
      "Test   Mean fold Loss: 5.5%\n",
      "Train Average accuracy: 67.1%\n",
      "Train Average loss: 1.4%\n",
      "Test Average accuracy: 57.8%\n",
      "Test Average loss: 5.0%\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "import torch as th\n",
    "from braindecode.experiments.monitors import compute_preds_per_trial_for_set\n",
    "rng = RandomState((2017,6,30))\n",
    "accuracy_nb = {\"Train\": 0, \"Test\": 0}\n",
    "accuracy_sum = {\"Train\": 0, \"Test\": 0}\n",
    "accuracyF_nb = {\"Train\": 0, \"Test\": 0}\n",
    "accuracyF_sum = {\"Train\": 0, \"Test\": 0}\n",
    "loss_nb = {\"Train\": 0, \"Test\": 0}\n",
    "loss_sum = {\"Train\": 0, \"Test\": 0}\n",
    "lossF_nb = {\"Train\": 0, \"Test\": 0}\n",
    "lossF_sum = {\"Train\": 0, \"Test\": 0}\n",
    "\n",
    "loss_graph = {\"Train\": [], \"Test\": []}\n",
    "accuracy_graph = {\"Train\": [], \"Test\": []}\n",
    "                  \n",
    "                  \n",
    "\n",
    "i_trial_stops = [trial.shape[1] for trial in train_set.X]\n",
    "\n",
    "for i_epoch in range(100):\n",
    "    all_acc = []\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    fold=0\n",
    "    for Train_index, Test_index in kf.split(train_set.X):\n",
    "        fold= fold+1\n",
    "        training_set = SignalAndTarget(train_set.X[train_index], y=y[train_index])\n",
    "        val_set = SignalAndTarget(train_set.X[test_index], y=y[test_index])\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        for batch_X, batch_y in iterator.get_batches(training_set, shuffle=False):\n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            net_target = np_to_var(batch_y)\n",
    "            if cuda:\n",
    "                net_target = net_target.cuda()\n",
    "            # Remove gradients of last backward pass from all parameters \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(net_in)\n",
    "            # Mean predictions across trial\n",
    "            # Note that this will give identical gradients to computing\n",
    "            # a per-prediction loss (at least for the combination of log softmax activation \n",
    "            # and negative log likelihood loss which we are using here)\n",
    "            outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "            loss = F.nll_loss(outputs, net_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print some statistics each epoch\n",
    "        model.eval()\n",
    "        print(\"Fold {:d}\".format(fold))\n",
    "        for setname, dataset in (('Train', training_set),('Test', val_set)):\n",
    "            \n",
    "            # Collect all predictions and losses\n",
    "            all_preds = []\n",
    "            all_losses = []\n",
    "            batch_sizes = []\n",
    "            for batch_X, batch_y in iterator.get_batches(dataset, shuffle=False):\n",
    "                net_in = np_to_var(batch_X)\n",
    "                if cuda:\n",
    "                    net_in = net_in.cuda()\n",
    "                net_target = np_to_var(batch_y)\n",
    "                if cuda:\n",
    "                    net_target = net_target.cuda()\n",
    "                outputs = model(net_in)\n",
    "                all_preds.append(var_to_np(outputs))\n",
    "                outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "                loss = F.nll_loss(outputs, net_target)\n",
    "                loss = float(var_to_np(loss))\n",
    "                all_losses.append(loss)\n",
    "                batch_sizes.append(len(batch_X))\n",
    "            # Compute mean per-input loss \n",
    "            loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "                           np.mean(batch_sizes))\n",
    "            print(\"{:6s} Loss: {:.5f}\".format(setname, loss))\n",
    "            lossF_nb[setname] += 1\n",
    "            lossF_sum[setname] += loss\n",
    "            # Assign the predictions to the trials\n",
    "            preds_per_trial = compute_preds_per_trial_for_set(all_preds,\n",
    "                                                              input_time_length,\n",
    "                                                              dataset)\n",
    "            # preds per trial are now trials x classes x timesteps/predictions\n",
    "            # Now mean across timesteps for each trial to get per-trial predictions\n",
    "            meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "            predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "            accuracy = np.mean(predicted_labels == dataset.y)\n",
    "                \n",
    "            print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "                setname, accuracy * 100))\n",
    "            accuracyF_nb[setname] += 1\n",
    "            accuracyF_sum[setname] += accuracy\n",
    "            \n",
    "            all_acc.append(accuracy * 100)\n",
    "    for setname in (\"Train\", \"Test\"):\n",
    "        print(\"{:6s} Mean fold Accuracy: {:.1f}%\".format(\n",
    "                    setname, accuracyF_sum[setname]*100/accuracyF_nb[setname]))\n",
    "        accuracy_graph[setname].append(accuracyF_sum[setname]/accuracyF_nb[setname])\n",
    "        \n",
    "        print(\"{:6s} Mean fold Loss: {:.1f}%\".format(\n",
    "                    setname, lossF_sum[setname]/lossF_nb[setname]))\n",
    "        loss_graph[setname].append(lossF_sum[setname]/lossF_nb[setname])\n",
    "        \n",
    "    for setname in (\"Train\", \"Test\"):        \n",
    "        accuracy_nb[setname] += 1\n",
    "        accuracy_sum[setname] += accuracyF_sum[setname]*100/accuracyF_nb[setname]\n",
    "        loss_nb[setname] += 1\n",
    "        loss_sum[setname] += lossF_sum[setname]/lossF_nb[setname]\n",
    "        \n",
    "for setname in (\"Train\", \"Test\"):\n",
    "    print(\"{} Average accuracy: {:.1f}%\".format(setname, accuracy_sum[setname]/accuracy_nb[setname]))\n",
    "    print(\"{} Average loss: {:.1f}%\".format(setname, loss_sum[setname]/loss_nb[setname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': [3.1128871917724608,\n",
       "  2.6922287583351134,\n",
       "  2.472870961825053,\n",
       "  2.317395123839378,\n",
       "  2.209186463356018,\n",
       "  2.1141305963198342,\n",
       "  2.0431839874812536,\n",
       "  1.9720189094543457,\n",
       "  1.9093775537278916,\n",
       "  1.8543364417552948,\n",
       "  1.7979708341034977,\n",
       "  1.749236690501372,\n",
       "  1.706585376079266,\n",
       "  1.6732367690120424,\n",
       "  1.6446731646855672,\n",
       "  1.6169210195541381,\n",
       "  1.6032981872558594,\n",
       "  1.588935054341952,\n",
       "  1.5674134320334385,\n",
       "  1.5485493311285972,\n",
       "  1.5300577680269878,\n",
       "  1.5136517532847145,\n",
       "  1.4999942113523899,\n",
       "  1.4896878210206828,\n",
       "  1.480576652288437,\n",
       "  1.4665378258778499,\n",
       "  1.4533999721209208,\n",
       "  1.4402411294834954,\n",
       "  1.4282160884347455,\n",
       "  1.4161482435464858,\n",
       "  1.4052054059120918,\n",
       "  1.393391509540379,\n",
       "  1.3838333984216054,\n",
       "  1.3761684847228668,\n",
       "  1.3674207775933402,\n",
       "  1.3589397561219003,\n",
       "  1.3497140879566605,\n",
       "  1.340704786620642,\n",
       "  1.3311240011300796,\n",
       "  1.3221461196243762,\n",
       "  1.3145802176580197,\n",
       "  1.307986143940971,\n",
       "  1.3013643824776937,\n",
       "  1.294258382239125,\n",
       "  1.2878288661109076,\n",
       "  1.2821357960286348,\n",
       "  1.2770992328511908,\n",
       "  1.2715919790168604,\n",
       "  1.2662399391738737,\n",
       "  1.2603383666276933,\n",
       "  1.2544461752854141,\n",
       "  1.2485879483131261,\n",
       "  1.242517688026968,\n",
       "  1.2352540710458049,\n",
       "  1.227077598029917,\n",
       "  1.2196357224668775,\n",
       "  1.2124324787081333,\n",
       "  1.2058472148303327,\n",
       "  1.1998907461004742,\n",
       "  1.194774394830068,\n",
       "  1.189513047796781,\n",
       "  1.1840720968861733,\n",
       "  1.1805008071755605,\n",
       "  1.1780288592912256,\n",
       "  1.1750974895403936,\n",
       "  1.1716965753923763,\n",
       "  1.167639187467632,\n",
       "  1.1638019403990578,\n",
       "  1.1601678162381268,\n",
       "  1.1572895809582302,\n",
       "  1.1539251403069832,\n",
       "  1.15026065732042,\n",
       "  1.1472725861693083,\n",
       "  1.143903162189432,\n",
       "  1.1407991789976755,\n",
       "  1.1375290225995214,\n",
       "  1.1340784040364351,\n",
       "  1.130463284254074,\n",
       "  1.127321090652973,\n",
       "  1.1237745122611522,\n",
       "  1.1206668499075336,\n",
       "  1.117145739095967,\n",
       "  1.113600670214159,\n",
       "  1.1103792929933185,\n",
       "  1.1075125854856829,\n",
       "  1.1047226013139237,\n",
       "  1.1017169130944657,\n",
       "  1.0985926130278545,\n",
       "  1.095935519290774,\n",
       "  1.0934057472811805,\n",
       "  1.0902165226228944,\n",
       "  1.0873770702792251,\n",
       "  1.0841067349398008,\n",
       "  1.0809092041659862,\n",
       "  1.0775786984594244,\n",
       "  1.0743985469763477,\n",
       "  1.0711534929644202,\n",
       "  1.068470209897781,\n",
       "  1.0656199289692774,\n",
       "  1.0629361776709556],\n",
       " 'Test': [2.8588666200637816,\n",
       "  3.178734505176544,\n",
       "  3.4538729429244994,\n",
       "  3.69587784409523,\n",
       "  3.838227047920227,\n",
       "  3.9918610135714214,\n",
       "  4.1720862422670635,\n",
       "  4.305473747849464,\n",
       "  4.394337677955628,\n",
       "  4.454341843128204,\n",
       "  4.4918676484714855,\n",
       "  4.5187684237957,\n",
       "  4.534555209600009,\n",
       "  4.557322238172803,\n",
       "  4.592163240114848,\n",
       "  4.621963985264301,\n",
       "  4.631521742484149,\n",
       "  4.656221518251631,\n",
       "  4.688823761438068,\n",
       "  4.723973881006241,\n",
       "  4.7419129655474705,\n",
       "  4.7580723209814595,\n",
       "  4.777511245271434,\n",
       "  4.797092216213544,\n",
       "  4.822282265663147,\n",
       "  4.845554297703963,\n",
       "  4.867084433414318,\n",
       "  4.882421511411667,\n",
       "  4.8961309934484545,\n",
       "  4.909007976055145,\n",
       "  4.922490748282402,\n",
       "  4.933392757922411,\n",
       "  4.946235339569323,\n",
       "  4.961934651346768,\n",
       "  4.976441386767796,\n",
       "  4.991228909624947,\n",
       "  5.004479818730741,\n",
       "  5.018481069489529,\n",
       "  5.032946751056573,\n",
       "  5.047194679379463,\n",
       "  5.0622010888122935,\n",
       "  5.075739931492579,\n",
       "  5.086364835362102,\n",
       "  5.094949063929644,\n",
       "  5.1042853127585515,\n",
       "  5.11612109567808,\n",
       "  5.13021758515784,\n",
       "  5.142803681393464,\n",
       "  5.154748263164443,\n",
       "  5.165247702121735,\n",
       "  5.175367787772534,\n",
       "  5.184857350587845,\n",
       "  5.1927662062195115,\n",
       "  5.1978486860239945,\n",
       "  5.2010959594899955,\n",
       "  5.205195451208524,\n",
       "  5.209384667664244,\n",
       "  5.2149578361675655,\n",
       "  5.223373386011286,\n",
       "  5.23235791405042,\n",
       "  5.240718832563181,\n",
       "  5.247857467974386,\n",
       "  5.255538303511483,\n",
       "  5.263627899065614,\n",
       "  5.270390191078186,\n",
       "  5.279472361550186,\n",
       "  5.28896771224577,\n",
       "  5.298753170756733,\n",
       "  5.3075423962828046,\n",
       "  5.3164060255459376,\n",
       "  5.324141940600436,\n",
       "  5.33076758020454,\n",
       "  5.337404625056541,\n",
       "  5.343974473347535,\n",
       "  5.350168072064718,\n",
       "  5.355876510080538,\n",
       "  5.360828549830945,\n",
       "  5.365688309608362,\n",
       "  5.371414776391621,\n",
       "  5.377214007675648,\n",
       "  5.3836259632934755,\n",
       "  5.389601523992492,\n",
       "  5.3953588040478255,\n",
       "  5.401487443844477,\n",
       "  5.407050435963799,\n",
       "  5.412845636245816,\n",
       "  5.417489216519498,\n",
       "  5.421839666637507,\n",
       "  5.426758576778884,\n",
       "  5.431838456524743,\n",
       "  5.435775785393767,\n",
       "  5.440117137587589,\n",
       "  5.444325780612166,\n",
       "  5.449249926272859,\n",
       "  5.452876784926967,\n",
       "  5.456768637150526,\n",
       "  5.460489958586152,\n",
       "  5.465227395904307,\n",
       "  5.469265375474487,\n",
       "  5.473162473917007]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': [0.5083333333333334,\n",
       "  0.5541666666666667,\n",
       "  0.576388888888889,\n",
       "  0.5885416666666667,\n",
       "  0.5950000000000001,\n",
       "  0.6,\n",
       "  0.6017857142857144,\n",
       "  0.6057291666666667,\n",
       "  0.6078703703703704,\n",
       "  0.6095833333333334,\n",
       "  0.6136363636363636,\n",
       "  0.6177083333333332,\n",
       "  0.6208333333333332,\n",
       "  0.6229166666666667,\n",
       "  0.625,\n",
       "  0.6283854166666667,\n",
       "  0.6301470588235294,\n",
       "  0.6314814814814815,\n",
       "  0.6344298245614035,\n",
       "  0.6370833333333333,\n",
       "  0.6386904761904765,\n",
       "  0.6399621212121217,\n",
       "  0.640760869565218,\n",
       "  0.6407986111111118,\n",
       "  0.6415000000000007,\n",
       "  0.6432692307692314,\n",
       "  0.644907407407408,\n",
       "  0.6470238095238102,\n",
       "  0.6488505747126444,\n",
       "  0.6511111111111119,\n",
       "  0.6532258064516138,\n",
       "  0.6555989583333344,\n",
       "  0.6575757575757587,\n",
       "  0.6590686274509816,\n",
       "  0.6608333333333346,\n",
       "  0.6627314814814829,\n",
       "  0.6645270270270285,\n",
       "  0.6662280701754402,\n",
       "  0.6683760683760699,\n",
       "  0.6704166666666681,\n",
       "  0.6723577235772372,\n",
       "  0.6740079365079378,\n",
       "  0.6752906976744196,\n",
       "  0.6765151515151523,\n",
       "  0.6776851851851857,\n",
       "  0.6788043478260873,\n",
       "  0.6796985815602838,\n",
       "  0.6806423611111112,\n",
       "  0.681547619047619,\n",
       "  0.6824166666666666,\n",
       "  0.683333333333333,\n",
       "  0.6842147435897431,\n",
       "  0.6852987421383642,\n",
       "  0.6865740740740736,\n",
       "  0.6878787878787874,\n",
       "  0.6889880952380948,\n",
       "  0.6900584795321633,\n",
       "  0.6910919540229881,\n",
       "  0.6920903954802256,\n",
       "  0.6930555555555551,\n",
       "  0.6939890710382509,\n",
       "  0.6948924731182792,\n",
       "  0.6956349206349202,\n",
       "  0.6962239583333328,\n",
       "  0.6970512820512816,\n",
       "  0.6978535353535349,\n",
       "  0.6986318407960195,\n",
       "  0.6993872549019603,\n",
       "  0.7001207729468595,\n",
       "  0.7008333333333329,\n",
       "  0.7015258215962437,\n",
       "  0.7021990740740737,\n",
       "  0.7028538812785384,\n",
       "  0.7034909909909906,\n",
       "  0.7041111111111107,\n",
       "  0.7047149122807014,\n",
       "  0.7053030303030299,\n",
       "  0.7059294871794868,\n",
       "  0.7064873417721514,\n",
       "  0.7070312499999996,\n",
       "  0.7075617283950613,\n",
       "  0.7080792682926825,\n",
       "  0.7086847389558228,\n",
       "  0.70922619047619,\n",
       "  0.7097549019607837,\n",
       "  0.7102228682170537,\n",
       "  0.7107279693486583,\n",
       "  0.7111742424242419,\n",
       "  0.7116104868913852,\n",
       "  0.7120370370370365,\n",
       "  0.7125457875457869,\n",
       "  0.712952898550724,\n",
       "  0.71353046594982,\n",
       "  0.7141400709219848,\n",
       "  0.714736842105262,\n",
       "  0.7153211805555542,\n",
       "  0.715850515463916,\n",
       "  0.7162840136054406,\n",
       "  0.71675084175084,\n",
       "  0.7172083333333316],\n",
       " 'Test': [0.5166666666666667,\n",
       "  0.5416666666666666,\n",
       "  0.5555555555555555,\n",
       "  0.5541666666666667,\n",
       "  0.5600000000000002,\n",
       "  0.563888888888889,\n",
       "  0.5666666666666667,\n",
       "  0.5687499999999999,\n",
       "  0.5703703703703701,\n",
       "  0.5716666666666663,\n",
       "  0.5727272727272723,\n",
       "  0.5736111111111108,\n",
       "  0.5743589743589743,\n",
       "  0.5750000000000001,\n",
       "  0.5755555555555558,\n",
       "  0.576041666666667,\n",
       "  0.5764705882352946,\n",
       "  0.5768518518518525,\n",
       "  0.577192982456141,\n",
       "  0.5775000000000008,\n",
       "  0.5777777777777786,\n",
       "  0.578030303030304,\n",
       "  0.5782608695652182,\n",
       "  0.5784722222222227,\n",
       "  0.578666666666667,\n",
       "  0.578846153846154,\n",
       "  0.5790123456790123,\n",
       "  0.5791666666666664,\n",
       "  0.5793103448275858,\n",
       "  0.5794444444444439,\n",
       "  0.5795698924731176,\n",
       "  0.5796874999999992,\n",
       "  0.5797979797979789,\n",
       "  0.5799019607843127,\n",
       "  0.5799999999999988,\n",
       "  0.5800925925925914,\n",
       "  0.5801801801801788,\n",
       "  0.5802631578947354,\n",
       "  0.5803418803418788,\n",
       "  0.580416666666665,\n",
       "  0.5804878048780471,\n",
       "  0.5805555555555538,\n",
       "  0.5806201550387579,\n",
       "  0.5806818181818163,\n",
       "  0.580740740740739,\n",
       "  0.5807971014492739,\n",
       "  0.580851063829786,\n",
       "  0.5809027777777768,\n",
       "  0.5809523809523802,\n",
       "  0.5809999999999994,\n",
       "  0.5810457516339865,\n",
       "  0.5810897435897434,\n",
       "  0.5811320754716981,\n",
       "  0.581172839506173,\n",
       "  0.5812121212121215,\n",
       "  0.5812500000000005,\n",
       "  0.581286549707603,\n",
       "  0.5813218390804605,\n",
       "  0.5813559322033908,\n",
       "  0.58138888888889,\n",
       "  0.5814207650273236,\n",
       "  0.5814516129032272,\n",
       "  0.5814814814814829,\n",
       "  0.5815104166666683,\n",
       "  0.5815384615384633,\n",
       "  0.5815656565656584,\n",
       "  0.581592039800997,\n",
       "  0.5816176470588256,\n",
       "  0.5816425120772969,\n",
       "  0.581666666666669,\n",
       "  0.5816901408450729,\n",
       "  0.5817129629629655,\n",
       "  0.5817351598173542,\n",
       "  0.5817567567567594,\n",
       "  0.5817777777777805,\n",
       "  0.5817982456140379,\n",
       "  0.5818181818181848,\n",
       "  0.5818376068376099,\n",
       "  0.5818565400843912,\n",
       "  0.5818750000000033,\n",
       "  0.5818930041152296,\n",
       "  0.5819105691056944,\n",
       "  0.5819277108433769,\n",
       "  0.5819444444444479,\n",
       "  0.5819607843137291,\n",
       "  0.5819767441860502,\n",
       "  0.5819923371647546,\n",
       "  0.5820075757575794,\n",
       "  0.5820224719101158,\n",
       "  0.5820370370370402,\n",
       "  0.582051282051285,\n",
       "  0.5820652173913071,\n",
       "  0.5820788530465975,\n",
       "  0.5820921985815626,\n",
       "  0.5821052631578968,\n",
       "  0.5821180555555574,\n",
       "  0.5821305841924415,\n",
       "  0.5821428571428585,\n",
       "  0.5821548821548833,\n",
       "  0.5821666666666676]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VPXZ//H3TchGEgKEsAgq4IILYgRBUYtxqRaxti59lMrTVmpprXX5Pda6dFFbre3T1r21rrU+Ki60Wq8ualWiuFFBKbKIFAmyE5aQhSQk5P79cWYjmYQAycwk+byu61wzZ507h+F85ny/Z86YuyMiItJUj2QXICIiqUkBISIicSkgREQkLgWEiIjEpYAQEZG4FBAiIhKXAkKkBWb2DzP7erLrEEkWBYR0aWZ2kpm9Y2bbzGyLmb1tZuPasq67T3L3P7bxddzMDm5l/mAze9HM1oaWHdZk/mNmtsPMqmKGtLa8tkhHUUBIl2VmvYG/AvcC/YAhwC1AXRLKaQReAs5vZZn/dffcmGFngmoTiUsBIV3ZoQDuPsPdd7p7jbu/4u4LAMzsZjN7IrywmQ0LfbrvGRovMbNLY+ZPM7MlZrbVzF42swND098MLfLv0Cf/C5sW4u4b3P13wPsd9+eKtC8FhHRlnwA7zeyPZjbJzPru7YbM7MvAjcB5QCEwG5gB4O4TQ4sdHfrk/8xevsx3Q81g88ystTMNkYRQQEiX5e4VwEmAAw8BZaF+gIF7sblvA7e7+xJ3bwB+DhSFzyLawT3AIcAA4MfAY2Z2YjttW2SvKCCkSwsd0L/h7kOBUcB+wF17sakDgbvNrNzMyoEtgBH0a7RHnR+4+2Z3b3D3vwNPEpytiCSNAkK6DXf/GHiMICgAqoFeMYsMamX1VcC33b1PzJDt7u90TLU4QQCJJI0CQrosMzvMzK4xs6Gh8f2BKcB7oUXmAxPN7AAzywduaGVzvwduMLMjQ9vKN7OvxMzfAIzYTT1ZQGZoNDM0Hp53gZnlmlkPMzsDmAq82OY/VqQDKCCkK6sEjgPmmFk1QTAsBK4BcPd/As8AC4B5BJfExuXuzwO/BJ42s4rQdibFLHIz8MdQE9R/tbCZGqAq9Pzj0HjYVcAaoBz4FfAtdy9p6x8q0hFMPxgkEl/o8tWH3f3xZNcikgw6gxCJw8x6ETQZrUh2LSLJ0mEBYWaPmtlGM1sYM62fmf3TzJaFHvf6unSRjmJmA4D1wBvAW0kuRyRpOvIM4jHgC02mXQ+85u6HAK+FxkVSirtvdPfe7n6xqw1WurEO7YMI3ZDsr+4+KjS+FCh293VmNhgocfeRHVaAiIjstZ4Jfr2B7r4OIBQSA1pa0MymA9MBsrKyxh5wwAEJKjG1NTY20qOHuo5A+yKW9kWU9kXUJ598ssndC/d2/UQHRJu5+4PAgwAjR470pUuXJrmi1FBSUkJxcXGyy0gJ2hdR2hdR2hdRZrZyX9ZPdMxuCDUtEXrcmODXFxGRNkp0QLwIhH+h6+vAXxL8+iIi0kYdeZnrDOBdYKSZrTazbwK/AD5vZsuAz4fGRUQkBXVYH4S7T2lh1mkd9Zoi0n7q6+tZvXo1tbW1yS5lj+Tn57NkyZJkl5FQWVlZDB06lPT09Hbdbsp2UotIcq1evZq8vDyGDRuGWee5sWxlZSV5eXnJLiNh3J3NmzezevVqhg8f3q7b1rVgIhJXbW0tBQUFnSocuiMzo6CgoEPO9BQQItIihUPn0FH/TgoIERGJSwEhIint+eefx8z4+OOPk1ZDeXk5v/vd7yLja9eu5YILLkhaPYmigBCRlDZjxgxOOukknn766XbZXkNDwx6v0zQg9ttvP2bOnNku9aQyBYSItMqs44eWVFVV8fbbb/PII49EAqKkpISJEydy7rnncsQRR/Cd73yHxsZGAHJzc7nxxhsZM2YMp512GmVlZQAUFxdz4403cvLJJ3P33XdTVlbG+eefz7hx4xg3bhxvv/02ADfffDPTpk2juLiYESNGcM899wBw/fXXs3z5coqKirj22mspLS1l1Kjgp80XLVrE+PHjKSoqYvTo0Sxbtozq6momT57M0UcfzahRo3jmmWcA+OlPf8q4ceMYNWoU06dPJ3yz1Pfff5/Ro0czYcIErr322si2d+7cybXXXsu4ceMYPXo0DzzwQDv/6+6Gu6f8cOihh7oEZs2alewSUob2RVRH7IvFixe7uzt0/NCS//u///Np06a5u/uECRN83rx5PmvWLM/MzPTly5d7Q0ODn3766f7cc8+FasUfeughd3e/5ZZb/PLLL3d395NPPtkvu+yyyHanTJnis2fPdnf3lStX+mGHHebu7jfddJNPmDDBa2trvayszPv16+c7duzwFStW+JFHHhlZP3b8e9/7nj/xxBPu7l5XV+fbt2/3mTNn+qWXXhpZvry83N3dN2/eHJk2depUf/HFF93d/cgjj/S3337b3d2vu+66yLYfeOAB/9nPfubu7rW1tT527Fj/9NNPW/33igXM9X049uoMQkRS1owZM7jooosAuOiii5gxYwYA48ePZ8SIEaSlpTFlyhTeeiv4XacePXpw/vnnAzB16tTIdIALL7ww8vzVV1/le9/7HkVFRZxzzjlUVFRQWVkJwOTJk8nMzKR///4MGDCADRs2tFrjhAkT+PnPf84vf/lLVq5cSXZ2NkcddRSvvvoq1113HbNnzyY/Px+AWbNmcdxxx3HUUUfx+uuvs2jRIsrLy6msrOSEE04A4Ktf/Wpk26+88gqPP/44RUVFHHfccWzevJlly5bt0z7dE/qinIikpM2bN/P666+zcOFCzIydO3diZpx11lnNLuts6TLP2Ok5OTmR542Njbz77rtkZ2c3WyczMzPyPC0tbbd9Fl/96lc57rjj+Nvf/saZZ57Jww8/zKmnnsq8efP4+9//zg033MAZZ5zBD37wA7773e8yd+5c9t9/f26++WZqa2sjzUzxuDv33nsvZ555Zqs1dBSdQYhISpo5cyZf+9rXWLlyJaWlpaxatYrhw4fz1ltv8a9//YsVK1bQ2NjIM888w0knnQQEB/4XXngBgKeeeioyvakzzjiD++67LzI+f/78VmvJy8uLnGE09emnnzJixAiuvPJKzjnnHBYsWMDatWvp1asXU6dO5fvf/z4ffPBB5Its/fv3p6qqKtLJ3bdvX/Ly8njvvfcAdumMP/PMM7n//vupr68H4JNPPqG6unq3+6696AxCRFqVrB9dnTFjBtdfv+uvEp9//vncf//9TJgwgeuvv56PPvoo0mENwVnCkiVLGDt2LPn5+ZHO4abuueceLr/8ckaPHk1DQwMTJ07k97//fYu1FBQUcOKJJzJq1CgmTZrE5ZdfHpn3zDPP8MQTT5Cens6gQYP4yU9+wvvvv8+1115Ljx49SE9P5/7776dPnz5861vf4qijjmLYsGGMGzcuso1HHnmEb33rW+Tk5FBcXBxpkrr00kspLS1lzJgxuDuFhYWRAEyIfenASNSgTuoodcxGaV9EdWQndaqZNWuWT548Oe68nJwcr6ioSHBF+66ysjLy/Pbbb/crr7xyj7fREZ3UOoMQEUmyv/3tb9x+++00NDRw4IEH8thjjyW7JEBNTCLSyRQXF7f4k6JVVVUt9hWksgsvvHCXq6xShTqpRUQkLgWEiIjEpYAQEZG4FBAiIhKXAkJEUlJxcTEvv/zyLtPuuusuvvvd77a4Tm5uLtD67biLi4uZO3duq6991113sX379sj4WWedRXl5eVtL7zIUECKSkqZMmdLsFt9PP/00U6ZM2e26+3o77qYB8fe//50+ffrs9fY6K13mKiKtsls6/mdH/abmX9e+4IIL+NGPfkRdXR2ZmZmUlpaydu1aioqKOO2009i6dSv19fXceuutfOlLX9pl3dLSUs4++2wWLlxITU0Nl1xyCYsXL+bwww+npqYmstxll13G+++/T01NDRdccAG33HIL99xzD2vXruWUU06hf//+zJo1i2HDhjF37lz69+/PHXfcwaOPPgoE33S++uqrKS0tZdKkSZx00km88847DBkyhL/85S9x7/XUmegMQkRSUkFBAePHj+ell14CgrOHCy+8kOzsbJ5//nk++OADZs2axTXXXNPqDe/uv/9+evXqxYIFC/jhD3/IvHnzIvNuu+025s6dy4IFC3jjjTdYsGABV155Jfvttx+zZs1i1qxZu2xr3rx5/OEPf2DOnDm89957PPTQQ3z44YcALFu2jMsvv5xFixbRp08f/vSnP3XAXkksBYSIpKzYZqZw85K7c+ONNzJ69GhOP/101qxZ0+otud98802mTp0KwOjRoxk9enRk3rPPPsuYMWM45phjWLRoEYsXL261nrfeeotzzz2XnJwccnNzOe+885g9ezYAw4cPp6ioCICxY8dSWlq6L396SlBAiEjK+vKXv8xrr73GBx98QE1NDWPGjOHJJ5+krKyMefPmMX/+fAYOHBi5U2pL4t0OfMWKFfz617/mtddeY8GCBUyePHm322ntTGVPbxPeGSggRCRl5ebmUlxczLRp0yKd09u2bWPAgAGkp6cza9YsVq5c2eo2Jk6cyJNPPgnAwoULWbBgAQAVFRXk5OSQn5/Phg0b+Mc//hFZp6Xbe0+cOJEXXniB7du3U11dzfPPP8/nPve59vpzU446qUWkVfE6kBNpypQpnHfeeZGmposvvpgvfvGLHHvssRQVFXHYYYe1uv5ll13GJZdcwujRoykqKmL8+PEAHH300RxzzDEceeSRjBgxghNPPDGyzvTp05k0aRKDBw/epR9izJgxfOMb34hs49JLL+WYY47pEs1J8Vhrp0ypYuTIkb506dJkl5ESSkpKWrxRWXejfRHVEftiyZIlHH744e26zUSorKwkLy8v2WUkXLx/LzOb5+7H7u021cQkIiJxKSBERCQuBYSItKgzNEFLx/07KSBEJK6srCw2b96skEhx7s7mzZvJyspq923rKiYRiWvo0KGsXr2asrKyZJeyR2prazvkYJnKsrKyGDp0aLtvVwEhInGlp6czfPjwZJexx0pKSjjmmGOSXUaXoCYmERGJKykBYWb/z8wWmdlCM5thZt3rfFBEpBNIeECY2RDgSuBYdx8FpAEXJboOERFpXbKamHoC2WbWE+gFrE1SHSIi0oKk3GrDzK4CbgNqgFfc/eI4y0wHpgMUFhaOffbZZxNbZIqqqqqK/Kxid6d9EaV9EaV9EXXKKafs0602Eh4QZtYX+BNwIVAOPAfMdPcnWlpH92KK0v2HorQvorQvorQvojrjvZhOB1a4e5m71wN/Bk5IQh0iItKKZATEZ8DxZtbLgl/xOA1YkoQ6RESkFQkPCHefA8wEPgA+CtXwYKLrEBGR1iXlm9TufhNwUzJeW0RE2kbfpBYRkbgUECIiEpcCQkRE4lJAiIhIXAoIERGJSwEhIiJxKSBERCQuBYSIiMSlgBARkbgUECIiEpcCQkRE4lJAiIhIXAoIERGJSwEhIiJxKSBERCQuBYSIiMSlgBARkbgUECIiEpcCQkRE4lJAiIhIXAoIERGJSwEhIiJxKSBERCQuBYSIiMSlgBARkbgUECIiEpcCQkRE4lJAiIhIXAoIERGJSwEhIiJxKSBERCQuBYSIiMSlgBARkbgUECIiEldSAsLM+pjZTDP72MyWmNmEZNQhIiIt65mk170beMndLzCzDKBXkuoQEZEWJDwgzKw3MBH4BoC77wB2JLoOERFpnbl7Yl/QrAh4EFgMHA3MA65y9+omy00HpgMUFhaOffbZZxNaZ6qqqqoiNzc32WWkBO2LKO2LKO2LqFNOOWWeux+7t+snIyCOBd4DTnT3OWZ2N1Dh7j9uaZ2RI0f60qVLE1ZjKispKaG4uDjZZaQE7Yso7Yso7YsoM9ungEhGJ/VqYLW7zwmNzwTGJKEOERFpRcIDwt3XA6vMbGRo0mkEzU0iIpJCknUV0xXAk6ErmD4FLklSHSIi0oKkBIS7zwf2ul1MREQ6nr5JLSIicSkgREQkLgWEiIjElaxOahGRbsHdqW+sp66hjrqdddQ11FHbUEtNQw11DXXs2LkjMtTtDMbjTa9tqI0MsctE1olZ95hBx/CbM3+zz7UrIESky3N3duzcwfb67bsMNQ01wWN9zS7Paxtqdzkohw/usQfpmoaa6PP6ml0O0OEgCD86if1Ccnu9ngJCRJLC3SMH2tiDdPgA3fRgXFNf0+ygvr1+O9sbgoN99Y5qttdvZ8OWDaQtSmsWBI3emOw/OWF27Gyf29spIERktxq9kaodVWyr3ca2um2Rx8q6Sip3VFJZV0nVjiqq66up3lEdPMY+j3ncXr+d6vpqauprOu6TdVXHbLazqGuoa5ftKCBEujB3p6ahhoq6imYH8Yq6il0O9uW15ZTXlkeeb63ZGhmvrKtMeDOJ7L2EnkGY2UEE90+qM7NiYDTwuLuXt0sVIhJRv7M+0ixSWVdJRV1Fs4N5eFp4WLluJZkrM6naUUXVjqpdPtXv9J3J/pO6vZ49epKRlkFmWiZZPbMiQ2bPTDLTMslIywjm94x5HjM9Iy2D7J7ZkeVj1w2vE14+s2cm+Zn57VN3G5f7E3CsmR0MPAK8CDwFnNUuVYh0Eo3eSEVdReTTdXltOVtrQ5+0a4MDd/jgvL1he7MOzPqd9btcmRKvQ3OvD+ib2/dv7Wp69uhJTnoOvdJ7kZ2eTU56Dtnp2WT3zCY7PZte6b2CeT2zowfwOAfj2PnZ6cHz2IN3+IAeu15aj7Rk//l7pa0B0ejuDWZ2LnCXu99rZh92ZGEiHc3dqdxRyYaqDWyo3kBZdRll28soqy5j0/ZNbKrZxKbtm9i8fTNbarawpWYL5bXlamppRxlpGc0OzOGDduzBOLNnJr169oocyLN7ZpOTkUN2z2A8JyMnsp2PF3zM547/XLMg6NlDLep7qq17rN7MpgBfB74YmpbeMSWJ7Jm6hjo210QP4ltrtjZrlgm3pW+p2RIc9Gs2U1ZdRk1DTbLL7zRy0nPondmb/Kx88jPzyc/Kp3dmb/Iy8sjLyCM3I5ecjJzIp/TY8djHXum9OvagXQpHDjiy/bfbDbX1X+cS4DvAbe6+wsyGA090XFnS3TU0NlBWXcb6qvVsqN7A+qr1rK9az9rKtayrWsf6qvWUlpVS+V4l2+q2JbvclJaZlhkcyDNDB/HQgTo3I5f8zHz6ZPWJPPbJ6kN+VvC8b1bfyLS8zDx9Au+G2vQv7u6LgSsBzKwvkOfuv+jIwqTraWhsYGP1RtZXrWdj9UY2VG1gY/XGYFr1+khTz/qq9ZRVl3Xbppwe1iPSjNI7s3fk4N47s3fwyT3m03v4E/zKZSs5fszx5GbkkpuRG3yqDwVCRlpGsv8k6aTaehVTCXBOaPn5QJmZveHu/9OBtUkn4O5U1FVQtr2MjdUbWVe5jrWVa3f5pL++aj3rqtZ1mYN+bkbuLp+u+2b3jXwKjzS5ZOZFmlHCnZ3hzsv0tPRmV5/EdoKm90jHzPaoppKtJRSPKO6Qv1e6r7aeM+a7e4WZXQr8wd1vMrMFHVmYJEejN7KlZgsbqjZEDvpl1cHjhurgE3+4DT/c5t9e11wnQ0ZaBoNyBzEwZyADcgZQmFNIYa9C+vfqH3ks6FVAQXYBfbP70jerL+lp6n6T7qGtAdHTzAYD/wX8sAPrkQ7k7pRtL2P5luX8Z8t/WL51OWsq1rC2ai3rKtdFPuV3tuvme1gPCrIL6Jfdj37Z/eib3TfSFJOXmRf5pB8eCrILIgf+/Mz8Pf60LtJdtDUgfgq8DLzt7u+b2QhgWceVJfuiobGB5VuWs7hsMUs2LWHJpiV8svkTlm5a2qk6dAuyCxiYO5BBuYMin/IH5w5mUO4gBucNZtWSVZxdfDb9svt12uvMRVJZWzupnwOeixn/FDi/o4qS1jU0NrCuch2fbfuM0vLSyLCifAWl5aV8tu0z6hvrk11mXP2y++1ywB+QM4DCXoXBeO5ABuYEgTAgZwCZPTNb3VbJZyUU5hQmqHKR7qetndRDgXuBEwEH3gKucvfVHVhbt1ZRV8Gc1XP4aONHfLbtMz7b9hmrKlbx6aZP2frG1pTq7M3umR1pux+YO5AheUPYL28/BucOZnDe4EgYDModtNuDvoikjrY2Mf2B4NYaXwmNTw1N+3xHFNXdNHojSzctZc6aOcxZPYd3V7/LRxs/StrtiXtn9mZAzoDIUNgrevAfmDOQwpxCCrILKOhVQN+svuRk5CSlThHpWG0NiEJ3/0PM+GNmdnVHFNQVNXojK7auYF3Vusj9e9ZWrg36B8qWsLhsMZU7KhNSS1bPLA7qexAH9zuYg/sdzLA+w9gvb7/IJ/4BOQPITs9OSC0iktraGhCbzGwqMCM0PgXdGiwud+ezbZ/xzqp3mLNmDh+s+4D56+cnLADCCnsVckThERxReASH9z+ckf1HMrJgJPvn708P00+Ri8jutTUgpgH3AXcS9EG8Q3D7jW4p/OWwtZVrWVO5huVblvPJ5k/4ZMsnfLDuA9ZWru3wGgqyCxjaeyjD+gyLDMP7DGd43+EM6zOM3pm9O7wGEena2noV02cE36SOCDUx3dURRaWSmvqayJnARxs/4qMNH/Hxpo+prq/u8Nc+qO9BTNh/Akf0P4ID8g/ggPwDWL14Ned+/lyyemZ1+OuLSPe2L3ff+h+6aEAs3LiQpxc+TUlpCf9a86+EXDKal5HHuCHjGL/feMYPGc+E/ScwKHdQs+VKVpQoHEQkIfYlILrU108r6yp5ZtEzPPzBw8xZM6fdt5+XkcfhhYdTkF0Q+UbvQX0P4vDCwzm8/+Ec2OdA9Q2ISErZl4BInQvx90FDYwO/n/t7biq5iS01W9plm+k90hkzeAwn7H8Cxw05jjGDx3BQv4MUACLSqbQaEGZWSfwgMKDTXwv5z+X/5OqXr2Zx2eI9XjczLTNyeej++ftzaL9DObQgGI4aeJSagUSk02s1INw9L1GFJFL9znqu+McVPDDvgTYtPzh3MBMPnMjYwWMZNWAURw08iiF5Q3STNxHp0rrdT0RV1FXwlee+wivLX2l1uaJBRUwrmsakQyZxUN+DFAYi0u10q4BYtW0Vk5+azEcbP2pxmf8e/d9cffzVjBk8JoGViYiknm4TECvLV3LioyeypnJN3PkThk7g7i/czbgh4xJcmYhIauoWAbGtdhuTn5ocNxx6WA/uPPNOrhh/hZqRRERidPmAaGhs4L9m/heLyhY1m5eTnsPTFzzN2YeenYTKRERSW9ICwszSgLnAGnfvkCO0u3PF36+I2yE9KHcQf/vq39TXICLSgmR+c+sqYElHvsDPZ/+c38/7fbPpuRm5vHTxSwoHEZFWJCUgQr9QNxl4uKNe47Y3b+NHs37UbHoP68HT5z/N0YOO7qiXFhHpEsw98XfMMLOZwO1AHvD9eE1MZjYdmA5QWFg49tlnn23z9p9Y+QSPlD4Sd94VB1/BeUPO25uyU0JVVRW5ubnJLiMlaF9EaV9EaV9EnXLKKfPc/di9XT/hfRBmdjaw0d3nmVlxS8u5+4PAgwAjR4704uIWF41dh1vfvLXFcLhy/JXcPenuvSk7ZZSUlNCWfdEdaF9EaV9EaV+0n2R0Up8InGNmZwFZQG8ze8Ldp+7LRhu9ke+/8n3ufO/OuPO/Pfbb3PmF+PNERKS5hPdBuPsN7j7U3YcBFwGv72s4NDQ2MO0v01oMh++M/Q6/m/w73U1VRGQPdPrvQdTvrOcrz32Fvyz9S9z53xn7HX47+bcKBxGRPZTUgHD3EqBkX7Zx65u3thgOVx13FXeceYfCQURkL3TqI+eijYu4/a3b48772Sk/484z71Q4iIjspU7bxNTojUz/6/RmvxdtGL8967dcNu6yJFUmItI1dNqP1w/MfYB3Vr3TbPoNJ92gcBARaQedMiDWVKzhulevazb9kH6H8OOTf5yEikREup5OGRBXv3w1lTsqm01/8IsP6regRUTaSacLiNkrZzNz8cxm0795zDcpHlac+IJERLqoThUQjd7INa9c02z6wJyB/Orzv0pCRSIiXVenCogZH83g/bXvN5v+i9N/Qd/svkmoSESk6+o0AVFTX8MNr93QbHrRoCK+dvTXklCRiEjX1mkC4o537mJVxapm039zxm/0ZTgRkQ7QKY6spasy+OmsnzebfvahZ3Pq8FOTUJGISNfXKb5JvaPnJrCqXaalWZo6pkVEOlCnOIMgZ2OzSd8e+20O639YEooREekeOkdAWOMuoxlpGfxw4g+TVIyISPfQOQKiiWlF09gvb79klyEi0qV1uoDoQRo/OPEHyS5DRKTL63QBMWjjVIb3HZ7sMkREurzOFRBurHvuBtasSXYhIiJdX+cKiMUX4GUjeeqpZBciItL1da6AmH0jAI8/Du5JrkVEpIvrPAGx9GxYXwTAwoXw738nuR4RkS6uUwREj4ZcmL3r9x5uuklnESIiHalTBER/OxBWH7/LtBdfhEceSVJBIiLdQKcIiN69G+jfv/n0q66CZcsSX4+ISHfQKQKiRw/nwQebT9++HaZOhfr6xNckItLVdYqAADj3XPjmN5tP/9e/4Prr1R8hItLeOk1AANx1Fxx0UPPpd9wBP/6xQkJEpD11qoDIzYUnnoC0tObzbrsNbr454SWJiHRZnSogAI4/Hm65Jf68n/4UfvADaGhIbE0iIl1RpwsIgBtvhP/5n/jzfvUrOOkk+M9/EluTiEhX0ykDwgx+/evgMtd45syBoiJ48EFobIy/jIiItK5TBgQEIXHnnXDFFfHnV1fDt78NJ58MixYltjYRka6g0wYEBCFx991Bn0SPFv6St94Kziauuw42b05sfSIinVmnDggIQuInP4HZs2HEiPjLNDTA//4vHHAAXHklrFiR2BpFRDqjhAeEme1vZrPMbImZLTKzFnoS9swJJ8D8+TBtWsvLbN8O994LBx8MU6bAhx+2xyuLiHRNyTiDaACucffDgeOBy83siPbYcF5ecAO/V1+FQw5pebnGRnj6aRgzBs44A156SZ3ZIiJNJTwg3H2du38Qel4JLAGGtOdrnHYaLFgQ9E1kZra+7D//CZMmBYHyy18iEOaFAAANTElEQVTCxo3tWYmISOdlnsT7U5jZMOBNYJS7VzSZNx2YDlBYWDj22Wef3avX2Lgxk5kzh/LXvw6mpqbnbpdPS2tk/PgtnH76Bk44YTNZWal1alFVVUVubm6yy0gJ2hdR2hdR2hdRp5xyyjx3P3Zv109aQJhZLvAGcJu7/7m1ZUeOHOlLly7dp9fbuhXuvx/uuQc2bGjbOrm5cMEF8LWvBZfLtnSlVCKVlJRQXFyc7DJSgvZFlPZFlPZFlJntU0Ak5ZBnZunAn4AndxcO7aVv3+Ab2KWl8MADQUf17lRVwWOPwamnwvDhcO218M476q8Qke4hGVcxGfAIsMTd70j062dlwfTp8PHH8MIL8IUvBJfK7s5nnwXf3j7xRBgyJNjGn/8M27Z1fM0iIsmQjDOIE4H/Bk41s/mh4axEF5GWBl/6EvzjH8F9m66/Pjjwt8X69fDQQ3D++VBQEITGtdfCzJmwapVuOy4iXcPue23bmbu/BbThM3vijBgBt98Ot94Kb7wBTz4Jzz0HlZW7X3fnzqDZ6Z13otMKC+Hoo4OhqAhGj4bDDoOMjI77G0RE2lvCAyKVpaUF/Q2nngr33QcvvgiPPw4vvxwEQVuVlQXfxXj11ei0nj2Dfo/Bg2HgwOgwaFAw7L9/EFS9erX/3yUisjcUEC3IzoYLLwyGsrIgLJ5/PvjexI4de769hoag3+Pjj1tfbsiQ4Ffzhg2DAw+MPh5wQBAiIiKJooBog8LC4Pewv/nNoNnp1VeDs4qXXw6uimpPa9YEw5tvxp/ft+8JDBsWBMmQIcFZyIABwWO/fsHQty/06RN8szwVLs0Vkc5JAbGH8vLg3HODwR2WL4f33gt+g2LOHPj3v/fuDKOttm7NYOvWtt1Hyiyot2/fXZu1wqEyYEAQfuHnBQVBU5iICCgg9olZ0K9w8MEwdWowrb4eli4NgmL+/OCWHwsWBFc+JZo7VFQEw8qVu1/eLAiT/v3jD+EzlH79gvGCgmBIT+/4v0VEEk8B0c7S02HUqGC4+OLo9LIyWL06uNfThg3BsH59MKxdC59+mvxLZN1hy5Zg+OSTtq+Xmxs0afXpA717B2ctubnBY35+MC38GB7y84OhT5/gMSenbd9HEZHEUUAkSGFhMLSmtjb4rYrS0uATf/gxPKxbl5rfsaiqCobVq/d+G2lp0eAIh01sgISHcPDk5cGyZX3Izg7CJXbIzlbfi0h7UECkkKwsOPzwYIhnxw7485/f5YADJrBmTRAY4TOSjRuD+02Fh/Ly4GdXO4udO6O1t11Ri3NycoIwyc0NnvfqtWuAxA5ZWc2nNR169YoO4fHs7CDYRLoqBUQnkpEBgwbVccIJbVu+oSG4FcjmzdFmrXCYlJUFz8vKgvFwwHQV1dXB0NYbM+6tjIxdwyN2CIdUbJNbuAkuPC3ekJOjfh1JDQqILqxnz2hH8qGH7n75+vqg/2HTpiA4Nm+OPg/3TYTnb94cDFu2pGazV6Ls2BEM5eXtu9309OgZT+yZS1bWrmc84eebNo3g9dej0zMygiE9PRjS0oL3Q3g8PMQul5kZPI99zMwM5ql/qHtSQEhEenr0Mti2amwMvhtSXh6cgVRUBP0RlZXRK6gqKoIzmfC0bduiQ3l58Fhb23F/V2dUXx/sm7YHzwEdWU6zMAmHTXg8PC92iBdGrT1vuq14240XbE0ft29Po7Y2GFdf1L5RQMg+6dEj2oF84IF7v526uiA8woGxbVsQOLFhUlERhEw4gNasKSc9vU+kOSk8KGzaX319MHSOfq3PRZ6lpbUcMLFBFzvEm9Z0nfAQu3zT0Gy6nbaGYrx1ktXXpYCQlJCZ2bYrvWKVlMyP+8MwO3dGw6KyMnjcvj36WFMTfaytDYaaml2H8Pym06qrd50vqW3nzui/X2dmFj+MmoZaOLiOOCK4j9y+UkBIlxO+ZLZ37+DmiB3FfdcAiQ2PcECFLwGurIwOsc1w4QALL1dVFYzrR6kklnu0vyuRFBAie8ks2oHcntyDJrfwGU94CJ/xhAMp9vmiRcsZPPggamqCdXfsCJqE6uqCT9HhJqKGhujz8DLh5+H1mj7W17fv3ycdr71umaOAEEkxZtGrlQoK2rZOSckqiosP6pB6Yj+9xoZK06GubtewiZ3X0ND6NmKnN10u3vZaWm7HDqit3UljY1rCP22nkva6TFoBISKtMote8toZlJTMpri4GPf4wdQ0oMJnVbFnV60Nscvt3Nl8vdjtNz1ja/ra8WqK93xPLyXXGYSISCvCHbvh75R0Vu7RIIoNnKbBFvvYXn+vAkJEJIWZRa9OSjR9jUREROJSQIiISFwKCBERiUsBISIicSkgREQkLgWEiIjEpYAQEZG4FBAiIhKXAkJEROJSQIiISFwKCBERiUsBISIicSkgREQkLgWEiIjEpYAQEZG4FBAiIhJXUgLCzL5gZkvN7D9mdn0yahARkdYlPCDMLA34LTAJOAKYYmZHJLoOERFpXTLOIMYD/3H3T919B/A08KUk1CEiIq1Ixm9SDwFWxYyvBo5rupCZTQemh0brzGxhAmrrDPoDm5JdRIrQvojSvojSvogauS8rJyMgLM40bzbB/UHgQQAzm+vux3Z0YZ2B9kWU9kWU9kWU9kWUmc3dl/WT0cS0Gtg/ZnwosDYJdYiISCuSERDvA4eY2XAzywAuAl5MQh0iItKKhDcxuXuDmX0PeBlIAx5190W7We3Bjq+s09C+iNK+iNK+iNK+iNqnfWHuzZr/RURE9E1qERGJTwEhIiJxpXRAdOdbcpjZ/mY2y8yWmNkiM7sqNL2fmf3TzJaFHvsmu9ZEMbM0M/vQzP4aGh9uZnNC++KZ0EUPXZ6Z9TGzmWb2cej9MaG7vi/M7P+F/n8sNLMZZpbVXd4XZvaomW2M/Y5YS+8DC9wTOpYuMLMxbXmNlA0I3ZKDBuAadz8cOB64PPT3Xw+85u6HAK+FxruLq4AlMeO/BO4M7YutwDeTUlXi3Q285O6HAUcT7JNu974wsyHAlcCx7j6K4KKXi+g+74vHgC80mdbS+2AScEhomA7c35YXSNmAoJvfksPd17n7B6HnlQQHgSEE++CPocX+CHw5ORUmlpkNBSYDD4fGDTgVmBlapFvsCzPrDUwEHgFw9x3uXk43fV8QXImZbWY9gV7AOrrJ+8Ld3wS2NJnc0vvgS8DjHngP6GNmg3f3GqkcEPFuyTEkSbUklZkNA44B5gAD3X0dBCECDEheZQl1F/ADoDE0XgCUu3tDaLy7vD9GAGXAH0LNbQ+bWQ7d8H3h7muAXwOfEQTDNmAe3fN9EdbS+2CvjqepHBBtuiVHV2dmucCfgKvdvSLZ9SSDmZ0NbHT3ebGT4yzaHd4fPYExwP3ufgxQTTdoToon1L7+JWA4sB+QQ9CU0lR3eF/szl79f0nlgOj2t+Qws3SCcHjS3f8cmrwhfGoYetyYrPoS6ETgHDMrJWhqPJXgjKJPqGkBus/7YzWw2t3nhMZnEgRGd3xfnA6scPcyd68H/gycQPd8X4S19D7Yq+NpKgdEt74lR6iN/RFgibvfETPrReDroedfB/6S6NoSzd1vcPeh7j6M4H3wurtfDMwCLggt1l32xXpglZmF79J5GrCYbvi+IGhaOt7MeoX+v4T3Rbd7X8Ro6X3wIvC10NVMxwPbwk1RrUnpb1Kb2VkEnxTDt+S4LcklJYyZnQTMBj4i2u5+I0E/xLPAAQT/Qb7i7k07qrosMysGvu/uZ5vZCIIzin7Ah8BUd69LZn2JYGZFBJ31GcCnwCUEH/a63fvCzG4BLiS46u9D4FKCtvUu/74wsxlAMcHtzTcANwEvEOd9EArQ+wiuetoOXOLuu73Ta0oHhIiIJE8qNzGJiEgSKSBERCQuBYSIiMSlgBARkbgUECIiElfCf1FOJFWY2U6Cy4jDnnb3XySrHpFUo8tcpdsysyp3z012HSKpSk1MIk2YWamZ/dLM/hUaDg5NP9DMXgvdT/81MzsgNH24mb1rZu+b2c/MrCo0vTj82xWh8fvM7Buh52PN7A0zm2dmL7flzpoiiaaAkO4s28zmxwwXxsyrcPfxBN8+vSs07T6CWyaPBp4E7glNv5vg5nnjgPW7e9HQPbbuBS5w97HAo0C3uUuAdB5qYpJuq6UmptBNAU91909DB/P17l5gZpuAwe5eH5q+zt37m9lmYFBoem9grbvnxt4WJLTd+4C5oeEdgttkQHArmXXufkYH/8kie0Sd1CLxeQvP92SZBnY9S88KPRqwyN0n7H15Ih1PTUwi8V0Y8/hu6Pk7BHeTBbgYeCv0/O0m08NWAkeYWaaZ5RPcbRRgKVBoZhMgaHIysyPb/08Q2Tc6g5DuLNvM5seMv+Tu4R/fyTSzOQQfoqaEpl0JPGpm1xL8qtsloelXAU+Z2VUEv98BgLuvMrNngQXAMoI7i+LuO8zsAuCeUHD0JOjnWNQRf6TI3lIfhEgToT6IY919016ur8tnpUtQE5OIiMSlMwgREYlLZxAiIhKXAkJEROJSQIiISFwKCBERiUsBISIicf1/gIOM/dkkkO0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch=[]\n",
    "for val in range(100):\n",
    "    epoch.append(val)\n",
    "\n",
    "t = (accuracy_graph['Train']*100)\n",
    "print(len(accuracy_graph['Train']))\n",
    "print(len(epoch))\n",
    "\n",
    "plt.title(\"Sujet 15\")\n",
    "plt.grid(True)\n",
    "plt.plot(epoch, accuracy_graph['Train'], \"b\", linewidth=5, label=\"Apprentissage\")\n",
    "plt.plot(epoch, accuracy_graph['Test'],\"g\", linewidth=5, label=\"Validation\")\n",
    "plt.axis([0, 100, 0, 1])\n",
    "plt.xlabel('Epoque')\n",
    "plt.ylabel('Prcision')\n",
    "plt.legend()    \n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Sujet 15\")\n",
    "plt.grid(True)\n",
    "plt.plot(epoch, loss_graph['Train'], \"b\", linewidth=5, label=\"Apprentissage\")\n",
    "plt.plot(epoch, loss_graph['Test'],\"g\", linewidth=5, label=\"Validation\")\n",
    "plt.axis([0, 100, 0, 10])\n",
    "plt.xlabel('Epoque')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 64, 497]) torch.Size([84])\n",
      "torch.FloatTensor\n",
      "==================================\n",
      "==================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'braindecode.torch_ext.optimizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5f9e37902ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbraindecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# these are good values for the deep model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'braindecode.torch_ext.optimizers'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# create model\n",
    "model_net = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    max_epochs=20,\n",
    "    batch_size=64,\n",
    "    #criterion=th.nn.NLLLoss(np_to_var(batch_X), np_to_var(batch_y), ignore_index=-100, reduce=None),\n",
    "    lr=0.1,\n",
    "#     device='cuda',  # uncomment this to train with CUDA\n",
    ")\n",
    "\n",
    "model_net.initialize()\n",
    "#model_net.initialize_module()\n",
    "#print(model_net)\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "# evaluate using 10-fold cross validation stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "print(np_to_var(X).size(), np_to_var(y).size() )\n",
    "\n",
    "for batch_X, batch_y in iterator.get_batches(train_set, shuffle=False):\n",
    "    pass\n",
    "    #scores = cross_val_score(model_net, batch_X, batch_y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "    \n",
    "#print(batch_X.type())\n",
    "print(np_to_var(batch_X).type())\n",
    "print(\"==================================\")\n",
    "#print(np_to_var(batch_X)[5].size() , np_to_var(batch_y).size())\n",
    "print(\"==================================\")\n",
    "#print(np_to_var(batch_X[5]).dim())\n",
    "\n",
    "param_grid = { \n",
    "    'param_1': [1, 2, 3, 4, 5],\n",
    "    'param_2': [20, 40, 60],\n",
    "    'param_3': [50, 100, 150]\n",
    "}\n",
    "\n",
    "\n",
    "#cv_model = GridSearchCV(estimator=model_net, param_grid=param_grid,scoring=\"accuracy\", cv=5)\n",
    "\n",
    "#cv_model.fit(np_to_var(batch_X), np_to_var(batch_y))\n",
    "\n",
    "A=np_to_var(batch_X)\n",
    "b=np_to_var(batch_y)\n",
    "\n",
    "\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "import torch.nn.functional as F\n",
    "optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "model.compile(loss=F.nll_loss, optimizer=optimizer,  iterator_seed=1, cropped=True)\n",
    "\n",
    "#scores = cross_val_score(model_net, np_to_var(batch_X[0]), np_to_var(batch_y[0]), scoring=\"accuracy\", cv=5)\n",
    "#print (\"Cross-validated scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model_net, X, y, cv=6)\n",
    "plt.scatter(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This dataset was created and contributed to PhysioNet by the developers of the [BCI2000](http://www.schalklab.org/research/bci2000) instrumentation system, which they used in making these recordings. The system is described in:\n",
    " \n",
    "     Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. (2004) BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE TBME 51(6):1034-1043.\n",
    "\n",
    "[PhysioBank](https://physionet.org/physiobank/) is a large and growing archive of well-characterized digital recordings of physiologic signals and related data for use by the biomedical research community and further described in:\n",
    "\n",
    "    Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000) PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220."
   ]
  }
 ],
 "metadata": {
  "git": {
   "keep_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
